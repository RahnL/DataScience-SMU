{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Data Mining - MSDS 7331 - Thurs 6:30, Summer 2016\n",
    "\n",
    "Team 3 (AKA Team Super Awesome):  Sal Melendez, Rahn Lieberman, Thomas Rogers\n",
    "\n",
    "Github page:\n",
    "https://github.com/RahnL/DataScience-SMU/tree/master/DataMining\n",
    "\n",
    "Note: Code borrowed heavily from Eric Larson's github pages for this class.\n",
    "https://github.com/eclarson/DataMiningNotebooks/blob/master/04.%20Logits%20and%20SVM.ipynb\n",
    "\n",
    "Code also borrowed from other projects we're working on using the same dataset.\n",
    "\n",
    "https://github.com/RahnL/DataScience-SMU/blob/master/DataMining/DataMining-MiniLab1-Lieberman-Melendez-Rogers.ipynb\n",
    "https://github.com/rlshuhart/MSDS6210-Immersion_Project/blob/master/Study/Closing%20the%20Gap%20Study%20Revisited.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "* Intro\n",
    "* Data Cleanup and Reduction\n",
    "\n",
    "**Classifications:**\n",
    "* Logistical Regression[\n",
    "* K-Nearest Neighbor, with and without PCA\n",
    "* Gradient Boosting Regression\n",
    "* Random Forest\n",
    "* Decision Tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "\n",
    "Our team has selected the 2014 Behavioral Risk Factor Surveillance System data (BRFSS), from the Center for Disease Control and prevention (CDC), to attempt to understand the relationship between quality of health and a number of behavioral, demographic and environmental factors. \n",
    "\n",
    "The purpose of the BRFSS project is to survey a large population of Americans on a wide range of topics to inform policy, research and healthcare delivery. The same or similar questions are asked each year and the resulting dataset gives not only a broad, comprehensive view of health quality in the United States, but it also provides a longitudinal view on how quality of care (among other factors) is changing over time.\n",
    "\n",
    "There are 279 variables in the dataset and over 460,000 surveys completed. The sheer breadth and complexity of this data, with missing, weighted and calculated variables requires a clear and distinct question of interest and some sense of what variables might help answer the question. We have chosen to focus on one particular question in the survey as our response variable and will attempt to better understand the impact reported behaviors have on responses to that question. \n",
    "\n",
    "Our response variable becomes the answer to the following question on quality of health: \"Would you say that in general your health is: (1) excellent, (2) very good, (3) good, (4) fair, (5) poor?\" (section 1.1, column 80)\n",
    "\n",
    "We reduce the 279 variables to focus on those related to behavioral survey questions. The corresponding variables from the questions related to behavior number 30, so our dataset is roughly 450,000 rows by 30 columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# plot graphs in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahnl\\Anaconda2\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2723: DtypeWarning: Columns (120) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting length is 464664 \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 464664 entries, 0 to 464663\n",
      "Columns: 279 entries, _STATE to RCSBIRTH\n",
      "dtypes: float64(226), int64(52), object(1)\n",
      "memory usage: 989.1+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_STATE</th>\n",
       "      <th>FMONTH</th>\n",
       "      <th>IDATE</th>\n",
       "      <th>IMONTH</th>\n",
       "      <th>IDAY</th>\n",
       "      <th>IYEAR</th>\n",
       "      <th>DISPCODE</th>\n",
       "      <th>SEQNO</th>\n",
       "      <th>_PSU</th>\n",
       "      <th>CTELENUM</th>\n",
       "      <th>...</th>\n",
       "      <th>_FOBTFS</th>\n",
       "      <th>_CRCREC</th>\n",
       "      <th>_AIDTST3</th>\n",
       "      <th>_IMPEDUC</th>\n",
       "      <th>_IMPMRTL</th>\n",
       "      <th>_IMPHOME</th>\n",
       "      <th>RCSBRAC1</th>\n",
       "      <th>RCSRACE1</th>\n",
       "      <th>RCHISLA1</th>\n",
       "      <th>RCSBIRTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1172014</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>2014</td>\n",
       "      <td>1100</td>\n",
       "      <td>2014000001</td>\n",
       "      <td>2014000001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1072014</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2014</td>\n",
       "      <td>1100</td>\n",
       "      <td>2014000002</td>\n",
       "      <td>2014000002</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1092014</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2014</td>\n",
       "      <td>1100</td>\n",
       "      <td>2014000003</td>\n",
       "      <td>2014000003</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1072014</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2014</td>\n",
       "      <td>1100</td>\n",
       "      <td>2014000004</td>\n",
       "      <td>2014000004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1162014</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>2014</td>\n",
       "      <td>1100</td>\n",
       "      <td>2014000005</td>\n",
       "      <td>2014000005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 279 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   _STATE  FMONTH    IDATE  IMONTH  IDAY  IYEAR  DISPCODE       SEQNO  \\\n",
       "0       1       1  1172014       1    17   2014      1100  2014000001   \n",
       "1       1       1  1072014       1     7   2014      1100  2014000002   \n",
       "2       1       1  1092014       1     9   2014      1100  2014000003   \n",
       "3       1       1  1072014       1     7   2014      1100  2014000004   \n",
       "4       1       1  1162014       1    16   2014      1100  2014000005   \n",
       "\n",
       "         _PSU  CTELENUM    ...     _FOBTFS  _CRCREC  _AIDTST3  _IMPEDUC  \\\n",
       "0  2014000001       1.0    ...         2.0      1.0       2.0         5   \n",
       "1  2014000002       1.0    ...         2.0      2.0       2.0         4   \n",
       "2  2014000003       1.0    ...         2.0      2.0       2.0         6   \n",
       "3  2014000004       1.0    ...         2.0      1.0       2.0         6   \n",
       "4  2014000005       1.0    ...         2.0      1.0       2.0         5   \n",
       "\n",
       "   _IMPMRTL  _IMPHOME  RCSBRAC1  RCSRACE1  RCHISLA1  RCSBIRTH  \n",
       "0         1         1       NaN       NaN       NaN       NaN  \n",
       "1         1         1       NaN       NaN       NaN       NaN  \n",
       "2         1         1       NaN       NaN       NaN       NaN  \n",
       "3         3         1       NaN       NaN       NaN       NaN  \n",
       "4         1         1       NaN       NaN       NaN       NaN  \n",
       "\n",
       "[5 rows x 279 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the dataset, and do some initial cleanup.\n",
    "\n",
    "df = pd.read_csv(\"data/LLCP2014XPT.txt\", sep=\"\\t\", encoding = \"ISO-8859-1\")\n",
    "df.head()\n",
    "\n",
    "print(\"Starting length is %.f \" % len(df))\n",
    "\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reduction and Pre-processing\n",
    "\n",
    "Because we're interested in the relationship between behaviors, demographics and other factors, and the impact they have on general health quality, we'll reduce the data frame down to those variables we think will have the biggest impact, including:\n",
    "\n",
    "#### Behaviors:\n",
    "- Whether someone smokes or not (represented by _SMOKER3)\n",
    "- Physical activity (represented by PHYSHLTH)\n",
    "\n",
    "#### Demographics:\n",
    "- Age (represented by _AGE_G)\n",
    "- Education level (represented by EDUCA)\n",
    "- Income level (represented by _INCOMG)\n",
    "- Race (represented by _IMPRACE, an imputed value based on the initial data ste)\n",
    "\n",
    "#### Other Factors:\n",
    "- The cost of health care (represented by MEDCOST)\n",
    "- Health coverage (represented by HLTHPLN1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'_AGE65YR'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-8d53195c8c31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Age 18 to 64 - Excludes 65 or older, refused, or missing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'_AGE65YR'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_AGE65YR'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Exclude blank, 'Don't know', 'Not Sure', or 'Refused'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'GENHLTH'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnotnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'GENHLTH'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rahnl\\Anaconda2\\lib\\site-packages\\pandas\\core\\frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1990\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1991\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1992\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1993\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1994\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rahnl\\Anaconda2\\lib\\site-packages\\pandas\\core\\frame.pyc\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1997\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1998\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1999\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2001\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rahnl\\Anaconda2\\lib\\site-packages\\pandas\\core\\generic.pyc\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1343\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1344\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1345\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1346\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rahnl\\Anaconda2\\lib\\site-packages\\pandas\\core\\internals.pyc\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   3223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3224\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3225\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3226\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3227\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rahnl\\Anaconda2\\lib\\site-packages\\pandas\\indexes\\base.pyc\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   1876\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1877\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1878\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1880\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas\\index.c:4027)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas\\index.c:3891)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas\\hashtable.c:12408)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas\\hashtable.c:12359)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '_AGE65YR'"
     ]
    }
   ],
   "source": [
    "# Age 18 to 64 - Excludes 65 or older, refused, or missing\n",
    "df = df[df['_AGE65YR'] == 1].drop('_AGE65YR', axis=1)\n",
    "\n",
    "# Exclude blank, 'Don't know', 'Not Sure', or 'Refused'\n",
    "df = df[((df['GENHLTH'].notnull()) & (~df['GENHLTH'].isin([7,9])))] \n",
    "\n",
    "# Reduce Ethnicity to White, Black, or Hispanic (ex. Asian 2%, American Indian/Alaskan Native 1.55%, other 2.8%)\n",
    "df = df[df['_IMPRACE'].isin([1,2,5])]\n",
    "# Has Health plan --Excludes 'Don't know', 'Not Sure', or 'Refused'. drops .6%\n",
    "df = df[df['HLTHPLN1'].isin([1,2])]\n",
    "\n",
    "# Translate GENHLTH to binary classification of\n",
    "# Combining the “excellent”, “very good” and “good” responses as measures of “good or better” (1) health \n",
    "# and the “fair” and “poor” measures as “fair and poor” (0).\n",
    "df.loc[(df['GENHLTH'] < 4), 'health'] = 1\n",
    "df.loc[(df['GENHLTH'] >= 4), 'health'] = 0\n",
    "\n",
    "# Extract survey year from sequence. IYEAR sometimes went into the next year. \n",
    "# This is one way to put designate the year of the data publication\n",
    "# Also, if we add  other years to the data, this seperates it.\n",
    "df['Rec_Year'] = df['SEQNO'].astype(str).str[:4].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 231507 entries, 2 to 464663\n",
      "Data columns (total 9 columns):\n",
      "health      231507 non-null float64\n",
      "_SMOKER3    231507 non-null float64\n",
      "PHYSHLTH    231507 non-null float64\n",
      "_AGE_G      231507 non-null int64\n",
      "EDUCA       231507 non-null float64\n",
      "_INCOMG     231507 non-null float64\n",
      "MEDCOST     231507 non-null float64\n",
      "HLTHPLN1    231507 non-null int64\n",
      "_IMPRACE    231507 non-null int64\n",
      "dtypes: float64(6), int64(3)\n",
      "memory usage: 17.7 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahnl\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\rahnl\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>health</th>\n",
       "      <th>_SMOKER3</th>\n",
       "      <th>PHYSHLTH</th>\n",
       "      <th>_AGE_G</th>\n",
       "      <th>EDUCA</th>\n",
       "      <th>_INCOMG</th>\n",
       "      <th>MEDCOST</th>\n",
       "      <th>HLTHPLN1</th>\n",
       "      <th>_IMPRACE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    health  _SMOKER3  PHYSHLTH  _AGE_G  EDUCA  _INCOMG  MEDCOST  HLTHPLN1  \\\n",
       "2      1.0       3.0      88.0       4    6.0      5.0      2.0         1   \n",
       "6      1.0       3.0       2.0       5    6.0      5.0      2.0         1   \n",
       "7      1.0       4.0       3.0       5    4.0      1.0      2.0         1   \n",
       "9      1.0       2.0       1.0       3    5.0      5.0      2.0         1   \n",
       "12     1.0       4.0      88.0       3    6.0      5.0      2.0         1   \n",
       "13     1.0       3.0      88.0       5    6.0      3.0      2.0         1   \n",
       "22     0.0       4.0      22.0       4    5.0      2.0      1.0         1   \n",
       "24     1.0       1.0      88.0       2    5.0      1.0      1.0         1   \n",
       "25     0.0       1.0      30.0       3    2.0      2.0      1.0         2   \n",
       "26     1.0       4.0      88.0       5    5.0      5.0      2.0         1   \n",
       "27     1.0       4.0       1.0       5    5.0      5.0      2.0         1   \n",
       "31     1.0       4.0      88.0       4    5.0      5.0      2.0         1   \n",
       "38     1.0       4.0      88.0       4    5.0      5.0      2.0         1   \n",
       "39     1.0       4.0      88.0       5    4.0      4.0      2.0         1   \n",
       "41     1.0       4.0      88.0       4    5.0      3.0      2.0         1   \n",
       "43     0.0       3.0      88.0       5    6.0      2.0      2.0         1   \n",
       "45     0.0       1.0      30.0       5    4.0      2.0      1.0         1   \n",
       "47     1.0       4.0      88.0       5    5.0      2.0      2.0         1   \n",
       "48     1.0       4.0      88.0       4    5.0      5.0      2.0         1   \n",
       "53     1.0       4.0      88.0       5    3.0      2.0      2.0         2   \n",
       "\n",
       "    _IMPRACE  \n",
       "2          1  \n",
       "6          1  \n",
       "7          1  \n",
       "9          1  \n",
       "12         1  \n",
       "13         1  \n",
       "22         1  \n",
       "24         1  \n",
       "25         1  \n",
       "26         1  \n",
       "27         1  \n",
       "31         1  \n",
       "38         1  \n",
       "39         1  \n",
       "41         1  \n",
       "43         1  \n",
       "45         1  \n",
       "47         1  \n",
       "48         1  \n",
       "53         1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the variables we want to look at to a new DF, and a little more cleanup\n",
    "df_reduced = df[['health','_SMOKER3','PHYSHLTH','_AGE_G','EDUCA','_INCOMG','MEDCOST','HLTHPLN1','_IMPRACE']]\n",
    "\n",
    "# Cleanup\n",
    "df_reduced.replace(7,np.nan, inplace=True)  #replace the \"refused\" answer choice\n",
    "df_reduced.replace(9, np.nan, inplace=True) #replace the 'Don't Know' choice\n",
    "df_reduced = df_reduced.dropna() # this drops those that were the refused/don't know.\n",
    "\n",
    "df_reduced.info()\n",
    "df_reduced.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='health', dtype='float64',\n",
       "       handle_unknown='error', n_values='auto', sparse=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = OneHotEncoder()\n",
    "enc.fit(df_reduced) \n",
    "OneHotEncoder(categorical_features='health', dtype='float64', handle_unknown='error', n_values='auto', sparse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>health</th>\n",
       "      <th>_SMOKER3</th>\n",
       "      <th>PHYSHLTH</th>\n",
       "      <th>_AGE_G</th>\n",
       "      <th>EDUCA</th>\n",
       "      <th>_INCOMG</th>\n",
       "      <th>MEDCOST</th>\n",
       "      <th>HLTHPLN1</th>\n",
       "      <th>_IMPRACE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    health  _SMOKER3  PHYSHLTH  _AGE_G  EDUCA  _INCOMG  MEDCOST  HLTHPLN1  \\\n",
       "2      1.0       3.0      88.0       4    6.0      5.0      2.0         1   \n",
       "6      1.0       3.0       2.0       5    6.0      5.0      2.0         1   \n",
       "7      1.0       4.0       3.0       5    4.0      1.0      2.0         1   \n",
       "9      1.0       2.0       1.0       3    5.0      5.0      2.0         1   \n",
       "12     1.0       4.0      88.0       3    6.0      5.0      2.0         1   \n",
       "13     1.0       3.0      88.0       5    6.0      3.0      2.0         1   \n",
       "22     0.0       4.0      22.0       4    5.0      2.0      1.0         1   \n",
       "24     1.0       1.0      88.0       2    5.0      1.0      1.0         1   \n",
       "25     0.0       1.0      30.0       3    2.0      2.0      1.0         2   \n",
       "26     1.0       4.0      88.0       5    5.0      5.0      2.0         1   \n",
       "\n",
       "    _IMPRACE  \n",
       "2          1  \n",
       "6          1  \n",
       "7          1  \n",
       "9          1  \n",
       "12         1  \n",
       "13         1  \n",
       "22         1  \n",
       "24         1  \n",
       "25         1  \n",
       "26         1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reduced.head(10)\n",
    "df_reduced1 = df_reduced\n",
    "df_reduced1.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Logistic Regression\n",
    "---\n",
    "\n",
    "Here, we are performing a logistic regression test to see how accurately we can predict health based on our chosen variables.\n",
    "\n",
    "We do 3-fold cross validation, using an 80/20 split for training and testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(231507, n_iter=3, test_size=0.2, random_state=None)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "\n",
    "# rerun with all variables, filling NaN values with zero\n",
    "# Create a copy of the dataframe, so the original is still available for other models in the notebook\n",
    "df_logreg = df_reduced\n",
    "df_logreg = df_logreg.fillna(value=0)\n",
    "\n",
    "#... setup x, y\n",
    "if 'health' in df_logreg:\n",
    "    y = df_logreg['health'].values # get the labels we want\n",
    "    del df_logreg['health'] # get rid of the class label\n",
    "\n",
    "X = df_logreg.values # use everything else to predict!\n",
    "\n",
    "# do the cross validation\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n=num_instances,\n",
    "                         n_iter=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "print (cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('====Iteration', 0, ' ====')\n",
      "('accuracy', 0.86430391775733229)\n",
      "('confusion matrix\\n', array([[ 2318,  4862],\n",
      "       [ 1421, 37701]]))\n",
      "('\\nAverage accuracy: ', 0.86430391775733229)\n",
      "('====Iteration', 1, ' ====')\n",
      "('accuracy', 0.86575093948425552)\n",
      "('confusion matrix\\n', array([[ 2301,  4855],\n",
      "       [ 1361, 37785]]))\n",
      "('\\nAverage accuracy: ', 0.86502742862079396)\n",
      "('====Iteration', 2, ' ====')\n",
      "('accuracy', 0.8644982938102026)\n",
      "('confusion matrix\\n', array([[ 2276,  4886],\n",
      "       [ 1388, 37752]]))\n",
      "('\\nAverage accuracy: ', 0.86485105035059684)\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "import datetime\n",
    "\n",
    "lr_clf = LogisticRegression(penalty='l2', C=1.0, class_weight=None) # get object\n",
    "\n",
    "iter_num = 0\n",
    "accuracy = 0\n",
    "\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object: \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "\n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test) # get test set predictions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print (\"====Iteration\",iter_num,\" ====\")\n",
    "    print (\"accuracy\", acc)\n",
    "    print (\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1\n",
    "    accuracy = accuracy + acc\n",
    "\n",
    "    print ('\\nAverage accuracy: ', accuracy/iter_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Note the above, accuracy is averaging around 86.5%.\n",
    "In previous work (mini-lab), we tested variations of L1 and L2, and changing other parameters, and it was approximately the same accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "Below we continue with the k-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import k-fold cross validation from scikit learn\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "if 'health' in df_logreg:\n",
    "    y = df_logreg['health'].values # get the labels we want\n",
    "    del df_logreg['health'] # get ride of the class label\n",
    "    X = df_logreg.values # use everything else to predict!\n",
    "    \n",
    "    \n",
    "KFoldCrossObject = KFold(len(y), n_folds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('accuracy:', 0.86091565265845094)\n",
      "[[ 3941  8438]\n",
      " [ 2295 62495]]\n",
      "(u'HLTHPLN1', 'has weight of', 0.10892370256472274)\n",
      "(u'_IMPRACE', 'has weight of', -0.11635031179149966)\n",
      "(u'_SMOKER3', 'has weight of', 0.18884822671609949)\n",
      "(u'MEDCOST', 'has weight of', 0.22758419728906032)\n",
      "(u'EDUCA', 'has weight of', 0.35870556755818855)\n",
      "(u'_AGE_G', 'has weight of', -0.51313862513090402)\n",
      "(u'_INCOMG', 'has weight of', 0.64900935164266571)\n",
      "(u'PHYSHLTH', 'has weight of', 0.73557191976136327)\n"
     ]
    }
   ],
   "source": [
    "for train_indices, test_indices in KFoldCrossObject: \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "\n",
    "# scale attributes by the training set\n",
    "scale = StandardScaler()\n",
    "scale.fit(X_train) # find scalings for each column that make this zero mean and unit std\n",
    "\n",
    "X_train_scaled = scale.transform(X_train) # apply to training\n",
    "X_test_scaled = scale.transform(X_test) # apply those means and std to the test set (without snooping at the test set values)\n",
    "\n",
    "# train the model just as before\n",
    "logReg = LogisticRegression(penalty='l2', C=0.05, n_jobs=-1) \n",
    "logReg.fit(X_train_scaled,y_train)  # train object\n",
    "\n",
    "y_hat = logReg.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf )\n",
    "\n",
    "# sort these attributes and spit them out\n",
    "zip_vars = zip(logReg.coef_.T,df_logreg.columns) # combine attributes\n",
    "zip_vars.sort(key = lambda t: np.abs(t[0])) # sort them by the magnitude of the weight\n",
    "for coef, name in zip_vars:\n",
    "    print(name, 'has weight of', coef[0]) # now print them out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy here is 86%, which is a in line with above, but our cross validation step was added here.\n",
    "Looking at our data, we can see the weights of the different variables in the text above, or graphed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA24AAAEvCAYAAAA9ypKHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x380610b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[15,4])\n",
    "\n",
    "weights = pd.Series(logReg.coef_[0],index=df_logreg.columns)\n",
    "weights.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Regression\n",
    "\n",
    "In the code below, we’ll explore gradient boosting regression. In looking at the graphic above, it looks like the amount of exercise someone gets has the biggest impact on quality of health. So we’ll select that as the explanatory variable and keep self-reported health quality as our response variable. We’ll set the number of regression trees to 500, the depth of each individual tree to 4 the loss function to least squares and the learning rate to .01. The resulting mean squared error is .0859, which indicates that the difference between the test and training sets is very small. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.0859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x4569db00>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAGJCAYAAACThGjuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15dd17b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# code built from: \n",
    "# http://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regression.html#example-ensemble-plot-gradient-boosting-regression-py\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "###############################################################################\n",
    "# Load data\n",
    "X, y = shuffle(df_reduced1.PHYSHLTH.reshape(-1,1), df_reduced1.health.ravel(), random_state=13)\n",
    "offset = int(X.shape[0] * 0.9)\n",
    "X_train, y_train = X[:offset], y[:offset]\n",
    "X_test, y_test = X[offset:], y[offset:]\n",
    "\n",
    "###############################################################################\n",
    "# Fit regression model\n",
    "params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 1,\n",
    "          'learning_rate': 0.01, 'loss': 'ls'}\n",
    "clf = ensemble.GradientBoostingRegressor(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "mse = mean_squared_error(y_test, clf.predict(X_test))\n",
    "print(\"MSE: %.4f\" % mse)\n",
    "\n",
    "###############################################################################\n",
    "# Plot training deviance\n",
    "\n",
    "# compute test set deviance\n",
    "test_score = np.zeros((params['n_estimators'],), dtype=np.float64)\n",
    "\n",
    "for i, y_pred in enumerate(clf.staged_predict(X_test)):\n",
    "    test_score[i] = clf.loss_(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Deviance')\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, clf.train_score_, 'b-',\n",
    "         label='Training Set Deviance')\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-',\n",
    "         label='Test Set Deviance')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Boosting Iterations')\n",
    "plt.ylabel('Deviance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# K-Nearest Neighbors\n",
    "---\n",
    "\n",
    "In this section, will do an prediction analysis based on k-nearest neighbors (KNN).\n",
    "This should give us clusters based on how well each point clusters on others.\n",
    "\n",
    "We will use 10-fold cross validation, and 10 nearest neighbors. For the neighbors, 10 is an arbitrary value, chosed because we have a really big dataset.  ** WE SHOULD VARY THIS TO SEE WHAT DIFFERENCE IT MAKES **\n",
    "We also use 10-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  1.,   3.,  88., ...,   2.,   1.,   1.],\n",
       "        [  1.,   3.,   2., ...,   2.,   1.,   1.],\n",
       "        [  1.,   4.,   3., ...,   2.,   1.,   1.],\n",
       "        ..., \n",
       "        [  0.,   3.,  20., ...,   2.,   1.,   5.],\n",
       "        [  1.,   4.,  88., ...,   2.,   1.,   5.],\n",
       "        [  1.,   4.,  88., ...,   2.,   1.,   5.]]),\n",
       " array([ 1.,  1.,  1., ...,  1.,  0.,  1.]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial setup\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "knn = df_reduced  # Copy the dataset for this analysis.\n",
    "knn.fillna(value=0)\n",
    "\n",
    "# and setup our X and Y\n",
    "if '_Health' in knn:\n",
    "    y = knn['_Health'].values # get the labels we want\n",
    "    del knn['_Health'] # get rid of the class label\n",
    "\n",
    "yhat = np.zeros(y.shape)  #empty array to fill with predictions m\n",
    "X = knn.values\n",
    "\n",
    "# Display the arrays.  Not really needed, but tells us that there are some nan values, which break things.\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data Cleanup. In conversion to numpy array from pandas\n",
    "# some values are being converted to nan or inf values.\n",
    "X=np.nan_to_num(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN accuracy 0.836376437861\n"
     ]
    }
   ],
   "source": [
    "#fit the KNN model to our data\n",
    "cv = StratifiedKFold(y, n_folds=10)\n",
    "\n",
    "# get a handle to the classifier object, which defines the type\n",
    "clf = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "for train, test in cv:\n",
    "    clf.fit(X[train],y[train])\n",
    "    yhat[test] = clf.predict(X[test])\n",
    "\n",
    "total_accuracy = mt.accuracy_score(y, yhat)\n",
    "print 'KNN accuracy', total_accuracy\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With n-folds = 3, accuracy was 84.13%\n",
    "\n",
    "with n-folds = 7, accuracy was 83.64%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# intentionally left blank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Pipeline with PCA and KNN\n",
    "---\n",
    "\n",
    "Wow, the KNN got 87.7% accuracy. That's really good. But is it accurate? Now trying a pipeline doing a PCA first, then the KNN again. (Again, with k=10.)\n",
    "Expect it will get even better, since PCA should eliminate whats no good. However, at this point we need to watch for overfitting. if our accuracy jumps real high, we should be suspicious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import RandomizedPCA \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# setup pipeline to take PCA, then fit a KNN classifier\n",
    "clf_pipe = Pipeline(\n",
    "    [('PCA',RandomizedPCA(n_components=300)),\n",
    "     ('CLF',KNeighborsClassifier(n_neighbors=10))]\n",
    ")\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "for train, test in cv:\n",
    "    clf_pipe.fit(X[train],y[train])\n",
    "    yhat[test] = clf_pipe.predict(X[test])\n",
    "\n",
    "total_accuracy = mt.accuracy_score(y, yhat)\n",
    "print 'KNN, pipeline accuracy', total_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's the same.  That's cool, since it's still using KNN.\n",
    "Let's try something else..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Random Forest Classifier\n",
    "--- \n",
    "Since two different versions of KNN came out the same, lets compare it to something else.  We'll try random forest in our pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-f67a8adb4d17>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m clf_pipe = Pipeline(\n\u001b[0m\u001b[0;32m      4\u001b[0m     [('PCA',RandomizedPCA(n_components=100)),\n\u001b[0;32m      5\u001b[0m      ('CLF',RandomForestClassifier(max_depth=50, n_estimators=150, n_jobs=-1))]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_pipe = Pipeline(\n",
    "    [('PCA',RandomizedPCA(n_components=100)),\n",
    "     ('CLF',RandomForestClassifier(max_depth=50, n_estimators=150, n_jobs=-1))]\n",
    ")\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "for train, test in cv:\n",
    "    clf_pipe.fit(X[train],y[train])\n",
    "    yhat[test] = clf_pipe.predict(X[test])\n",
    "    \n",
    "total_accuracy = mt.accuracy_score(y, yhat)\n",
    "print 'Pipeline accuracy %0.2f (+/- %0.2f)' % total_accuracy % (total_accuracy.std()*2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very suspicious because it's so high. Seems likely we are overfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
