{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Data Mining - MSDS 7331 - Thurs 6:30, Summer 2016\n",
    "\n",
    "Team 3 (AKA Team Super Awesome):  Sal Melendez, Rahn Lieberman, Thomas Rogers\n",
    "\n",
    "Github page:\n",
    "https://github.com/RahnL/DataScience-SMU/tree/master/DataMining\n",
    "\n",
    "(Note: Code borrowed heavily from Eric Larson's github pages for this class.\n",
    "https://github.com/eclarson/DataMiningNotebooks/blob/master/04.%20Logits%20and%20SVM.ipynb)\n",
    "\n",
    "<hr>\n",
    "\n",
    "# Data Mining - Mini Lab 1\n",
    "\n",
    "This lab continues on our exploration and analysis of the BRFSS dataset, which are the results of a phone interviews with approximately 464,000 people in 2014.\n",
    "All the fields are either likert scales or boolean, with the caveat that almost all of the results contain an extra two values to indicate answers of \"Don't Know\" or \"No Response\".  That is, a binary variable may have 4 results.\n",
    "\n",
    "The first step is importing the data into a Pandas dataframe and clean it up to get our variables of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahnl\\Anaconda2\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2723: DtypeWarning: Columns (120) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_STATE</th>\n",
       "      <th>FMONTH</th>\n",
       "      <th>IDATE</th>\n",
       "      <th>IMONTH</th>\n",
       "      <th>IDAY</th>\n",
       "      <th>IYEAR</th>\n",
       "      <th>DISPCODE</th>\n",
       "      <th>SEQNO</th>\n",
       "      <th>_PSU</th>\n",
       "      <th>CTELENUM</th>\n",
       "      <th>...</th>\n",
       "      <th>_FOBTFS</th>\n",
       "      <th>_CRCREC</th>\n",
       "      <th>_AIDTST3</th>\n",
       "      <th>_IMPEDUC</th>\n",
       "      <th>_IMPMRTL</th>\n",
       "      <th>_IMPHOME</th>\n",
       "      <th>RCSBRAC1</th>\n",
       "      <th>RCSRACE1</th>\n",
       "      <th>RCHISLA1</th>\n",
       "      <th>RCSBIRTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1172014</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>2014</td>\n",
       "      <td>1100</td>\n",
       "      <td>2014000001</td>\n",
       "      <td>2014000001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1072014</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2014</td>\n",
       "      <td>1100</td>\n",
       "      <td>2014000002</td>\n",
       "      <td>2014000002</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1092014</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2014</td>\n",
       "      <td>1100</td>\n",
       "      <td>2014000003</td>\n",
       "      <td>2014000003</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1072014</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2014</td>\n",
       "      <td>1100</td>\n",
       "      <td>2014000004</td>\n",
       "      <td>2014000004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1162014</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>2014</td>\n",
       "      <td>1100</td>\n",
       "      <td>2014000005</td>\n",
       "      <td>2014000005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 279 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   _STATE  FMONTH    IDATE  IMONTH  IDAY  IYEAR  DISPCODE       SEQNO  \\\n",
       "0       1       1  1172014       1    17   2014      1100  2014000001   \n",
       "1       1       1  1072014       1     7   2014      1100  2014000002   \n",
       "2       1       1  1092014       1     9   2014      1100  2014000003   \n",
       "3       1       1  1072014       1     7   2014      1100  2014000004   \n",
       "4       1       1  1162014       1    16   2014      1100  2014000005   \n",
       "\n",
       "         _PSU  CTELENUM    ...     _FOBTFS  _CRCREC  _AIDTST3  _IMPEDUC  \\\n",
       "0  2014000001       1.0    ...         2.0      1.0       2.0         5   \n",
       "1  2014000002       1.0    ...         2.0      2.0       2.0         4   \n",
       "2  2014000003       1.0    ...         2.0      2.0       2.0         6   \n",
       "3  2014000004       1.0    ...         2.0      1.0       2.0         6   \n",
       "4  2014000005       1.0    ...         2.0      1.0       2.0         5   \n",
       "\n",
       "   _IMPMRTL  _IMPHOME  RCSBRAC1  RCSRACE1  RCHISLA1  RCSBIRTH  \n",
       "0         1         1       NaN       NaN       NaN       NaN  \n",
       "1         1         1       NaN       NaN       NaN       NaN  \n",
       "2         1         1       NaN       NaN       NaN       NaN  \n",
       "3         3         1       NaN       NaN       NaN       NaN  \n",
       "4         1         1       NaN       NaN       NaN       NaN  \n",
       "\n",
       "[5 rows x 279 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "\n",
    "df = pd.read_csv(\"data/LLCP2014XPT.txt\", sep=\"\\t\", encoding = \"ISO-8859-1\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying the Variables of Interest\n",
    "We are most interested to know how the variables in our dataset relate to self-reported health quality.\n",
    "\n",
    "We'll work to reduce the dataset and create an imputed variable from the self-reported measure of health.\n",
    "\n",
    "The question of interest is, \"Would You Say in General That Your Health is: (1) excellent, (2) very good, (3) good, (4) fair, (5) poor.\" Choices 7 and 9 were \"unsure\" and \"not asked\", respectively. This is in the GENHLTH variable.\n",
    "\n",
    "Response variables of interest:\n",
    "\n",
    "* _AGE80: Calculated continuous variable for imputed age, values above 80 are collapsed. Any respondent over 80 is included in the 80 year old bracket. \n",
    "* _INCOMG: Calculated variable for income categories. \n",
    "    (1) Less than \\$15,000, (2) \\$15,000-\\$24,999, (3) \\$25,000-\\$34,9999, (4) \\$35,000 - \\$49,999, (5) $50,000 or more (9) Don't Know\n",
    "* _SMOKER3:  Calculated variable for four-level smoker status:  (1)everyday smoker, (2) someday smoker, (3) former smoker, (4) non-smoker, (9) Don't Know\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahnl\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\rahnl\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 380915 entries, 0 to 464663\n",
      "Data columns (total 4 columns):\n",
      "_AGE80      380915 non-null int64\n",
      "_INCOMG     380915 non-null float64\n",
      "_SMOKER3    380915 non-null float64\n",
      "health      380915 non-null category\n",
      "dtypes: category(1), float64(2), int64(1)\n",
      "memory usage: 12.0 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_AGE80</th>\n",
       "      <th>_INCOMG</th>\n",
       "      <th>_SMOKER3</th>\n",
       "      <th>health</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>67</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _AGE80  _INCOMG  _SMOKER3 health\n",
       "0      61      5.0       3.0      0\n",
       "1      73      2.0       4.0      0\n",
       "2      52      5.0       3.0      1\n",
       "3      67      3.0       4.0      0\n",
       "4      67      2.0       4.0      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reduced = df[['GENHLTH','_AGE80','_INCOMG', '_SMOKER3']]\n",
    "\n",
    "# Cleanup\n",
    "df_reduced.replace(7,np.nan, inplace=True)  #replace the \"refused\" answer choice\n",
    "df_reduced.replace(9, np.nan, inplace=True) #replace the 'Don't Know' choice\n",
    "df_reduced = df_reduced.dropna() # this drops those that were the refused/don't know.\n",
    "\n",
    "# this creates a new variable by categorizing GENHLTH into three categories\n",
    "df_reduced['health'] = pd.cut(df_reduced.GENHLTH,[0,2,5],2,labels=[1,0]) \n",
    "\n",
    "del df_reduced['GENHLTH']\n",
    "\n",
    "df_reduced.info()\n",
    "df_reduced.head()\n",
    "# Genhlth, _incomg, _smoker3 are all categorical and integer already, so one-hot-encoding is not needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training And Testing\n",
    "\n",
    "Threefold cross validation, using 80% of data as training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(380915, n_iter=3, test_size=0.2, random_state=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import ShuffleSplit\n",
    "\n",
    "#... setup x, y\n",
    "#if '_Age80' in df_reduced:\n",
    "y = df_reduced['health'].values # get the labels we want\n",
    "del df_reduced['health'] # get rid of the class label\n",
    "\n",
    "X = df_reduced.values # use everything else to predict!\n",
    "\n",
    "# do the cross validation\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n=num_instances,\n",
    "                         n_iter=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "print (cv_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression \n",
    "\n",
    "Now that the data is loaded into memory, we'll do a logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Starting: ', datetime.datetime(2016, 6, 24, 21, 43, 38, 972000))\n",
      "('====Iteration', 10, ' ====')\n",
      "('accuracy', 0.64997440373836679)\n",
      "('confusion matrix\\n', array([[21386, 15892],\n",
      "       [10774, 28131]]))\n",
      "('Ending: ', datetime.datetime(2016, 6, 24, 21, 43, 39, 673000))\n",
      "('Elapsed Time: ', datetime.timedelta(0, 0, 701000))\n",
      "('Starting: ', datetime.datetime(2016, 6, 24, 21, 43, 39, 689000))\n",
      "('====Iteration', 11, ' ====')\n",
      "('accuracy', 0.64712599923867531)\n",
      "('confusion matrix\\n', array([[21500, 16046],\n",
      "       [10837, 27800]]))\n",
      "('Ending: ', datetime.datetime(2016, 6, 24, 21, 43, 40, 417000))\n",
      "('Elapsed Time: ', datetime.timedelta(0, 0, 728000))\n",
      "('Starting: ', datetime.datetime(2016, 6, 24, 21, 43, 40, 433000))\n",
      "('====Iteration', 12, ' ====')\n",
      "('accuracy', 0.65157581087644223)\n",
      "('confusion matrix\\n', array([[21541, 15860],\n",
      "       [10684, 28098]]))\n",
      "('Ending: ', datetime.datetime(2016, 6, 24, 21, 43, 41, 161000))\n",
      "('Elapsed Time: ', datetime.timedelta(0, 0, 728000))\n",
      "('\\nAverage accuracy: ', 0.14989817029642188)\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "import datetime\n",
    "\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "lr_clf = LogisticRegression(penalty='l2', C=1.0, class_weight=None) # get object\n",
    "\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "iter_num = 10\n",
    "accuracy = 0\n",
    "\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object: \n",
    "\n",
    "    starttime = datetime.datetime.now() \n",
    "    print ('Starting: ', starttime)\n",
    "    \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print (\"====Iteration\",iter_num,\" ====\")\n",
    "    print (\"accuracy\", acc)\n",
    "    print (\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1\n",
    "    accuracy = accuracy + acc\n",
    "    \n",
    "    endtime = datetime.datetime.now()\n",
    "    print ('Ending: ', endtime)\n",
    "    print ('Elapsed Time: ', endtime - starttime)\n",
    "    \n",
    "print ('\\nAverage accuracy: ', accuracy/iter_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Notice the last line above, giving the average accuracy for all the iterations.\n",
    "\n",
    "### Interpretation of Weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'_AGE80', 'has weight of', -0.016867742231118851)\n",
      "(u'_INCOMG', 'has weight of', 0.40882451236165862)\n",
      "(u'_SMOKER3', 'has weight of', 0.24073613821946188)\n"
     ]
    }
   ],
   "source": [
    "weights = lr_clf.coef_.T # take transpose to make a column vector\n",
    "variable_names = df_reduced.columns\n",
    "for coef, name in zip(weights,variable_names):\n",
    "    print(name, 'has weight of', coef[0])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we analyze the weights, itâ€™s important to recall again the scale of the answer choices that created this data. Age is not categorized, itâ€™s taken as a continuous variable and everyone 80 years or older is lumped together. So with age, smaller is younger, is better. With income, there are five answer choices: (1) Less than \\$15,000, (2) \\$15,000-\\$24,999, (3) \\$25,000-\\$34,9999, (4) \\$35,000 - \\$49,999, (5) \\$50,000 or more. With this variable, higher is better. \n",
    "\n",
    "With the final variable, _SMOKER3, the four choices are: (1) everyday smoker, (2) someday smoker, (3) former smoker, (4) non-smoker. With this variable, higher is better. \n",
    "\n",
    "With this context in mind, letâ€™s look at the resulting weights. Age appears to have the smallest impact on the model and is negatively correlated with perceived health. Each year of age increase results in .017 reduction in the likelihood that you will self-report good health. The next variable, income, has the largest impact on self-reported health and as you would expect, as income increases, so to does the likelihood that you will report good health. The final variable is smoking and the positive correlation may seem strange, if you donâ€™t consider the structure of the response variables above. The higher the answer choice, the less someone smokes, the less someone smokes, the higher the liklihood that they self-report good health. \n",
    "\n",
    "These weights and correlations make logical sense. We can take increased income as a signal of more options. More options related to choices that everyone knows has a positive impact on health, like food, exercise and healthcare. The impact of smoking on health is well documented and unfortunately, we also know that as we age, our quality of life decreases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('accuracy:', 0.65157581087644223)\n",
      "[[21541 15860]\n",
      " [10684 28098]]\n",
      "(u'_SMOKER3', 'has weight of', 0.23463130993303205)\n",
      "(u'_AGE80', 'has weight of', -0.27755892283989814)\n",
      "(u'_INCOMG', 'has weight of', 0.59892212037823345)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# we want to normalize the features based upon the mean and standard deviation of each column. \n",
    "# However, we do not want to accidentally use the testing data to find out the mean and std (this would be snooping)\n",
    "# to Make things easier, let's start by just using whatever was last stored in the variables:\n",
    "##    X_train , y_train , X_test, y_test (they were set in a for loop above)\n",
    "\n",
    "# scale attributes by the training set\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train) # find scalings for each column that make this zero mean and unit std\n",
    "# the line of code above only looks at training data to get mean and std and we can use it \n",
    "# to transform new feature data\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) # apply those means and std to the test set (without snooping at the test set values)\n",
    "\n",
    "# train the model just as before\n",
    "lr_clf = LogisticRegression(penalty='l2', C=0.05) # get object, the 'C' value is less (can you guess why??)\n",
    "lr_clf.fit(X_train_scaled,y_train)  # train object\n",
    "\n",
    "y_hat = lr_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf )\n",
    "\n",
    "\n",
    "# sort these attributes and spit them out\n",
    "zip_vars = zip(lr_clf.coef_.T,df_reduced.columns) # combine attributes\n",
    "zip_vars.sort(key = lambda t: np.abs(t[0])) # sort them by the magnitude of the weight\n",
    "for coef, name in zip_vars:\n",
    "    print(name, 'has weight of', coef[0]) # now print them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEwCAYAAABbv6HjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMSHJX1MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x174a0e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now let's make a pandas Series with the names and values, and plot them\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "weights = pd.Series(lr_clf.coef_[0],index=df_reduced.columns)\n",
    "weights.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point it would make sense to remove variables that are highly related to one another or ones that are irrelevant and keep going with the weights analysis.  \n",
    "For this analysis, we removee GENHLTH above, which is what we're trying to predict, and have as the 'health' variable in a seperate dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEwCAYAAABbv6HjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x174d2c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# you can also apply the StandardScaler function insied of the validation loop \n",
    "#  but this requires the use of PipeLines in scikit. Here is an example, but we will go over more \n",
    "#  thorough examples later in class\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "std_scl = StandardScaler()\n",
    "lr_clf = LogisticRegression(penalty='l2', C=0.05) \n",
    "\n",
    "# create the pipline\n",
    "piped_object = Pipeline([('scale', std_scl), ('logit_model', lr_clf)])\n",
    "\n",
    "# run the pipline crossvalidated\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object):\n",
    "    piped_object.fit(X[train_indices],y[train_indices])  # train object\n",
    "    \n",
    "# it is a little odd getting trained objects from a  pipeline:\n",
    "trained_model_from_pipeline = piped_object.named_steps['logit_model']\n",
    "\n",
    "# now look at the weights\n",
    "weights = pd.Series(trained_model_from_pipeline.coef_[0],index=df_reduced.columns)\n",
    "weights.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We know this takes a while to run, so we'll benchmark it for giggles\n",
    "print datetime.datetime.now() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# okay, so run through the cross validation loop and set the training and testing variable for one single iteration\n",
    "for train_indices, test_indices in cv_object: \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) \n",
    "\n",
    "print ('Training endeding: ', starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lets investigate SVMs on the data and play with the parameters and kernels\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# train the model just as before\n",
    "svm_clf = SVC(C=0.05, kernel='linear', degree=3, gamma='auto') # get object\n",
    "svm_clf.fit(X_train_scaled, y_train)  # train object\n",
    "\n",
    "y_hat = svm_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print (\"accuracy:\", acc) \n",
    "print (conf)\n",
    "\n",
    "print ('accuracy calculations endeding: ', starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# look at the support vectors\n",
    "print (svm_clf.support_vectors_.shape)\n",
    "print (svm_clf.support_.shape)\n",
    "print (svm_clf.n_support_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if using linear kernel, these make sense to look at (not otherwise, why?)\n",
    "print svm_clf.coef_\n",
    "weights = pd.Series(svm_clf.coef_[0],index=df_reduced.columns)\n",
    "weights.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now let's do some different analysis with the SVM and look at the instances that were chosen as support vectors\n",
    "\n",
    "# now lets look at the support for the vectors and see if we they are indicative of anything\n",
    "# grabe the rows that were selected as support vectors (these are usually instances that are hard to classify)\n",
    "\n",
    "\n",
    "# make a dataframe of the training data\n",
    "df_tested_on = df_reduced.iloc[train_indices] # saved from above, the indices chosen for training\n",
    "# now get the support vectors from the trained model\n",
    "df_support = df_tested_on.iloc[svm_clf.support_,:]\n",
    "\n",
    "df_support['health'] = y[svm_clf.support_] # add back in the 'health' Column to the pandas dataframe\n",
    "df_reduced['health'] = y # also add it back in for the original data\n",
    "df_support.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now lets see the statistics of these attributes\n",
    "starttime = datetime.datetime.now() \n",
    "print (\"**\", startTime)\n",
    "from pandas.tools.plotting import boxplot\n",
    "\n",
    "# group the original data and the support vectors\n",
    "df_grouped_support = df_support.groupby(['health'])\n",
    "df_grouped = df_reduced.groupby(['health'])\n",
    "\n",
    "# plot KDE of Different variables\n",
    "vars_to_plot = ['_AGE80', '_INCOMG','_SMOKER3']\n",
    "\n",
    "for v in vars_to_plot:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    # plot support vector stats\n",
    "    plt.subplot(1,2,1)\n",
    "    ax = df_grouped_support[v].plot.kde() \n",
    "    plt.legend(['_SMOKER3','_INCOMG'])\n",
    "    plt.title(v+' (Instances chosen as Support Vectors)')\n",
    "    \n",
    "    # plot original distributions\n",
    "    plt.subplot(1,2,2)\n",
    "    ax = df_grouped[v].plot.kde() \n",
    "    plt.legend(['_SMOKER3','_INCOMG'])\n",
    "    plt.title(v+' (Original)')\n",
    "\n",
    "print ('** Endeding: ', starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
