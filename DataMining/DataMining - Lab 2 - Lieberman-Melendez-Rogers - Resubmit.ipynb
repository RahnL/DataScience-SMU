{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Data Mining - MSDS 7331 - Thurs 6:30, Summer 2016\n",
    "\n",
    "Team 3 (AKA Team Super Awesome):  Sal Melendez, Rahn Lieberman, Thomas Rogers\n",
    "\n",
    "Github page:\n",
    "https://github.com/RahnL/DataScience-SMU/tree/master/DataMining\n",
    "\n",
    "Note: Code borrowed heavily from Eric Larson's github pages for this class.\n",
    "https://github.com/eclarson/DataMiningNotebooks/blob/master/04.%20Logits%20and%20SVM.ipynb\n",
    "\n",
    "Code also borrowed from other projects we're working on using the same dataset.\n",
    "\n",
    "https://github.com/RahnL/DataScience-SMU/blob/master/DataMining/DataMining-MiniLab1-Lieberman-Melendez-Rogers.ipynb\n",
    "https://github.com/rlshuhart/MSDS6210-Immersion_Project/blob/master/Study/Closing%20the%20Gap%20Study%20Revisited.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "* Intro\n",
    "* Data Cleanup and Reduction\n",
    "\n",
    "**Classifications:**\n",
    "* Logistical Regression\n",
    "* K-Nearest Neighbor, with and without PCA\n",
    "* Random Forest\n",
    "* Gradient Boosting Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "\n",
    "Our team has selected the 2014 Behavioral Risk Factor Surveillance System data (BRFSS), from the Center for Disease Control and prevention (CDC), to attempt to understand the relationship between quality of health and a number of behavioral, demographic and environmental factors. \n",
    "\n",
    "The purpose of the BRFSS project is to survey a large population of Americans on a wide range of topics to inform policy, research and healthcare delivery. The same or similar questions are asked each year and the resulting dataset gives not only a broad, comprehensive view of health quality in the United States, but it also provides a longitudinal view on how quality of care (among other factors) is changing over time.\n",
    "\n",
    "There are 279 variables in the dataset and over 460,000 surveys completed. The sheer breadth and complexity of this data, with missing, weighted and calculated variables requires a clear and distinct question of interest and some sense of what variables might help answer the question. We have chosen to focus on one particular question in the survey as our response variable and will attempt to better understand the impact reported behaviors have on responses to that question. \n",
    "\n",
    "Our response variable becomes the answer to the following question on quality of health: \"Would you say that in general your health is: (1) excellent, (2) very good, (3) good, (4) fair, (5) poor?\" (section 1.1, column 80)\n",
    "\n",
    "We reduce the 279 variables to focus on those related to behavioral survey questions. The corresponding variables from the questions related to behavior number 30, so our dataset is roughly 450,000 rows by 30 columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# plot graphs in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trogers\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2723: DtypeWarning: Columns (120) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting length is 464664 \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 464664 entries, 0 to 464663\n",
      "Columns: 279 entries, _STATE to RCSBIRTH\n",
      "dtypes: float64(226), int64(52), object(1)\n",
      "memory usage: 989.1+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_STATE</th>\n",
       "      <th>FMONTH</th>\n",
       "      <th>IDATE</th>\n",
       "      <th>IMONTH</th>\n",
       "      <th>IDAY</th>\n",
       "      <th>IYEAR</th>\n",
       "      <th>DISPCODE</th>\n",
       "      <th>SEQNO</th>\n",
       "      <th>_PSU</th>\n",
       "      <th>CTELENUM</th>\n",
       "      <th>...</th>\n",
       "      <th>_FOBTFS</th>\n",
       "      <th>_CRCREC</th>\n",
       "      <th>_AIDTST3</th>\n",
       "      <th>_IMPEDUC</th>\n",
       "      <th>_IMPMRTL</th>\n",
       "      <th>_IMPHOME</th>\n",
       "      <th>RCSBRAC1</th>\n",
       "      <th>RCSRACE1</th>\n",
       "      <th>RCHISLA1</th>\n",
       "      <th>RCSBIRTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1172014</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>2014</td>\n",
       "      <td>1100</td>\n",
       "      <td>2014000001</td>\n",
       "      <td>2014000001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1072014</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2014</td>\n",
       "      <td>1100</td>\n",
       "      <td>2014000002</td>\n",
       "      <td>2014000002</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1092014</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2014</td>\n",
       "      <td>1100</td>\n",
       "      <td>2014000003</td>\n",
       "      <td>2014000003</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1072014</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2014</td>\n",
       "      <td>1100</td>\n",
       "      <td>2014000004</td>\n",
       "      <td>2014000004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1162014</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>2014</td>\n",
       "      <td>1100</td>\n",
       "      <td>2014000005</td>\n",
       "      <td>2014000005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 279 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   _STATE  FMONTH    IDATE  IMONTH  IDAY  IYEAR  DISPCODE       SEQNO  \\\n",
       "0       1       1  1172014       1    17   2014      1100  2014000001   \n",
       "1       1       1  1072014       1     7   2014      1100  2014000002   \n",
       "2       1       1  1092014       1     9   2014      1100  2014000003   \n",
       "3       1       1  1072014       1     7   2014      1100  2014000004   \n",
       "4       1       1  1162014       1    16   2014      1100  2014000005   \n",
       "\n",
       "         _PSU  CTELENUM    ...     _FOBTFS  _CRCREC  _AIDTST3  _IMPEDUC  \\\n",
       "0  2014000001       1.0    ...         2.0      1.0       2.0         5   \n",
       "1  2014000002       1.0    ...         2.0      2.0       2.0         4   \n",
       "2  2014000003       1.0    ...         2.0      2.0       2.0         6   \n",
       "3  2014000004       1.0    ...         2.0      1.0       2.0         6   \n",
       "4  2014000005       1.0    ...         2.0      1.0       2.0         5   \n",
       "\n",
       "   _IMPMRTL  _IMPHOME  RCSBRAC1  RCSRACE1  RCHISLA1  RCSBIRTH  \n",
       "0         1         1       NaN       NaN       NaN       NaN  \n",
       "1         1         1       NaN       NaN       NaN       NaN  \n",
       "2         1         1       NaN       NaN       NaN       NaN  \n",
       "3         3         1       NaN       NaN       NaN       NaN  \n",
       "4         1         1       NaN       NaN       NaN       NaN  \n",
       "\n",
       "[5 rows x 279 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the dataset, and do some initial cleanup.\n",
    "\n",
    "df = pd.read_csv(\"data/LLCP2014XPT.txt\", sep=\"\\t\", encoding = \"ISO-8859-1\")\n",
    "df.head()\n",
    "\n",
    "print(\"Starting length is %.f \" % len(df))\n",
    "\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reduction and Pre-processing\n",
    "\n",
    "We're interested in the relationship between behaviors, demographics and other factors, and the impact they have on general health quality. For this reason, we'll narrow down the dataset to those variables that we believe are most pertinent. Smoking and the level of physical activity have a demonstrated impact on health quality and they’re behaviors. Age has an obvious impact, but education, income level and race may have a less obvious impact. The cost and access to care have been identified as factors that negatively impact health, so we’ll include those as well. Our model ultimately will show the relative impact these variables have on health quality. \n",
    "\n",
    "#### Behaviors:\n",
    "- Whether someone smokes or not (represented by _SMOKER3)\n",
    "- Physical activity (represented by PHYSHLTH)\n",
    "\n",
    "#### Demographics:\n",
    "- Age (represented by _AGE_G)\n",
    "- Education level (represented by EDUCA)\n",
    "- Income level (represented by _INCOMG)\n",
    "- Race (represented by _IMPRACE, an imputed value based on the initial data ste)\n",
    "\n",
    "#### Other Factors:\n",
    "- The cost of health care (represented by MEDCOST)\n",
    "- Health coverage (represented by HLTHPLN1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation Part 2\n",
    "\n",
    "After analyzing the distribution of a number of variables, the age variable is contains a large percentage of people that are over 65. Fearing the skewness this will have on our data, we narrow the dataset to only contain those 18 to 64. \n",
    "\n",
    "Related to our variable of interest, we removed those responses where the answer provided was “don’t know”, “not sure” or “refused”. We did the same for the variable “HLTHPLN1”. Because they were such small percentage of the population, we imputed a variable for race, by removing those who identified as Asian, American Indian/Alaskan Native or Other. \n",
    "\n",
    "In order to allow for logistic regression, we translated our variable of interest from five variables “excellent”, “very good”, “good”, “fair” and “poor” to a binary variable, “good or better” and “fair or worse”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Age 18 to 64 - Excludes 65 or older, refused, or missing\n",
    "df = df[df['_AGE65YR'] == 1].drop('_AGE65YR', axis=1)\n",
    "\n",
    "# Exclude blank, 'Don't know', 'Not Sure', or 'Refused'\n",
    "df = df[((df['GENHLTH'].notnull()) & (~df['GENHLTH'].isin([7,9])))] \n",
    "\n",
    "# Reduce Ethnicity to White, Black, or Hispanic (ex. Asian 2%, American Indian/Alaskan Native 1.55%, other 2.8%)\n",
    "df = df[df['_IMPRACE'].isin([1,2,5])]\n",
    "# Has Health plan --Excludes 'Don't know', 'Not Sure', or 'Refused'. drops .6%\n",
    "df = df[df['HLTHPLN1'].isin([1,2])]\n",
    "\n",
    "# Translate GENHLTH to binary classification of\n",
    "# Combining the “excellent”, “very good” and “good” responses as measures of “good or better” (1) health \n",
    "# and the “fair” and “poor” measures as “fair and poor” (0).\n",
    "df.loc[(df['GENHLTH'] < 4), 'health'] = 1\n",
    "df.loc[(df['GENHLTH'] >= 4), 'health'] = 0\n",
    "\n",
    "# Extract survey year from sequence. IYEAR sometimes went into the next year. \n",
    "# This is one way to put designate the year of the data publication\n",
    "# Also, if we add  other years to the data, this seperates it.\n",
    "df['Rec_Year'] = df['SEQNO'].astype(str).str[:4].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trogers\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\trogers\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 231507 entries, 2 to 464663\n",
      "Data columns (total 9 columns):\n",
      "health      231507 non-null float64\n",
      "_SMOKER3    231507 non-null float64\n",
      "PHYSHLTH    231507 non-null float64\n",
      "_AGE_G      231507 non-null int64\n",
      "EDUCA       231507 non-null float64\n",
      "_INCOMG     231507 non-null float64\n",
      "MEDCOST     231507 non-null float64\n",
      "HLTHPLN1    231507 non-null int64\n",
      "_IMPRACE    231507 non-null int64\n",
      "dtypes: float64(6), int64(3)\n",
      "memory usage: 17.7 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>health</th>\n",
       "      <th>_SMOKER3</th>\n",
       "      <th>PHYSHLTH</th>\n",
       "      <th>_AGE_G</th>\n",
       "      <th>EDUCA</th>\n",
       "      <th>_INCOMG</th>\n",
       "      <th>MEDCOST</th>\n",
       "      <th>HLTHPLN1</th>\n",
       "      <th>_IMPRACE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    health  _SMOKER3  PHYSHLTH  _AGE_G  EDUCA  _INCOMG  MEDCOST  HLTHPLN1  \\\n",
       "2      1.0       3.0      88.0       4    6.0      5.0      2.0         1   \n",
       "6      1.0       3.0       2.0       5    6.0      5.0      2.0         1   \n",
       "7      1.0       4.0       3.0       5    4.0      1.0      2.0         1   \n",
       "9      1.0       2.0       1.0       3    5.0      5.0      2.0         1   \n",
       "12     1.0       4.0      88.0       3    6.0      5.0      2.0         1   \n",
       "13     1.0       3.0      88.0       5    6.0      3.0      2.0         1   \n",
       "22     0.0       4.0      22.0       4    5.0      2.0      1.0         1   \n",
       "24     1.0       1.0      88.0       2    5.0      1.0      1.0         1   \n",
       "25     0.0       1.0      30.0       3    2.0      2.0      1.0         2   \n",
       "26     1.0       4.0      88.0       5    5.0      5.0      2.0         1   \n",
       "27     1.0       4.0       1.0       5    5.0      5.0      2.0         1   \n",
       "31     1.0       4.0      88.0       4    5.0      5.0      2.0         1   \n",
       "38     1.0       4.0      88.0       4    5.0      5.0      2.0         1   \n",
       "39     1.0       4.0      88.0       5    4.0      4.0      2.0         1   \n",
       "41     1.0       4.0      88.0       4    5.0      3.0      2.0         1   \n",
       "43     0.0       3.0      88.0       5    6.0      2.0      2.0         1   \n",
       "45     0.0       1.0      30.0       5    4.0      2.0      1.0         1   \n",
       "47     1.0       4.0      88.0       5    5.0      2.0      2.0         1   \n",
       "48     1.0       4.0      88.0       4    5.0      5.0      2.0         1   \n",
       "53     1.0       4.0      88.0       5    3.0      2.0      2.0         2   \n",
       "\n",
       "    _IMPRACE  \n",
       "2          1  \n",
       "6          1  \n",
       "7          1  \n",
       "9          1  \n",
       "12         1  \n",
       "13         1  \n",
       "22         1  \n",
       "24         1  \n",
       "25         1  \n",
       "26         1  \n",
       "27         1  \n",
       "31         1  \n",
       "38         1  \n",
       "39         1  \n",
       "41         1  \n",
       "43         1  \n",
       "45         1  \n",
       "47         1  \n",
       "48         1  \n",
       "53         1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the variables we want to look at to a new DF, and a little more cleanup\n",
    "df_reduced = df[['health','_SMOKER3','PHYSHLTH','_AGE_G','EDUCA','_INCOMG','MEDCOST','HLTHPLN1','_IMPRACE']]\n",
    "\n",
    "# Cleanup\n",
    "df_reduced.replace(7,np.nan, inplace=True)  #replace the \"refused\" answer choice\n",
    "df_reduced.replace(9, np.nan, inplace=True) #replace the 'Don't Know' choice\n",
    "df_reduced = df_reduced.dropna() # this drops those that were the refused/don't know.\n",
    "\n",
    "df_reduced.info()\n",
    "df_reduced.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='health', dtype='float64',\n",
       "       handle_unknown='error', n_values='auto', sparse=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = OneHotEncoder()\n",
    "enc.fit(df_reduced) \n",
    "OneHotEncoder(categorical_features='health', dtype='float64', handle_unknown='error', n_values='auto', sparse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>health</th>\n",
       "      <th>_SMOKER3</th>\n",
       "      <th>PHYSHLTH</th>\n",
       "      <th>_AGE_G</th>\n",
       "      <th>EDUCA</th>\n",
       "      <th>_INCOMG</th>\n",
       "      <th>MEDCOST</th>\n",
       "      <th>HLTHPLN1</th>\n",
       "      <th>_IMPRACE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    health  _SMOKER3  PHYSHLTH  _AGE_G  EDUCA  _INCOMG  MEDCOST  HLTHPLN1  \\\n",
       "2      1.0       3.0      88.0       4    6.0      5.0      2.0         1   \n",
       "6      1.0       3.0       2.0       5    6.0      5.0      2.0         1   \n",
       "7      1.0       4.0       3.0       5    4.0      1.0      2.0         1   \n",
       "9      1.0       2.0       1.0       3    5.0      5.0      2.0         1   \n",
       "12     1.0       4.0      88.0       3    6.0      5.0      2.0         1   \n",
       "13     1.0       3.0      88.0       5    6.0      3.0      2.0         1   \n",
       "22     0.0       4.0      22.0       4    5.0      2.0      1.0         1   \n",
       "24     1.0       1.0      88.0       2    5.0      1.0      1.0         1   \n",
       "25     0.0       1.0      30.0       3    2.0      2.0      1.0         2   \n",
       "26     1.0       4.0      88.0       5    5.0      5.0      2.0         1   \n",
       "\n",
       "    _IMPRACE  \n",
       "2          1  \n",
       "6          1  \n",
       "7          1  \n",
       "9          1  \n",
       "12         1  \n",
       "13         1  \n",
       "22         1  \n",
       "24         1  \n",
       "25         1  \n",
       "26         1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reduced.head(10)\n",
    "df_reduced1 = df_reduced\n",
    "df_reduced1.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Logistic Regression\n",
    "---\n",
    "\n",
    "Here, we are performing a logistic regression test to see how accurately we can predict health based on our chosen variables.\n",
    "\n",
    "We do 3-fold cross validation, using an 80/20 split for training and testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(231507, n_iter=10, test_size=0.2, random_state=None)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "\n",
    "# rerun with all variables, filling NaN values with zero\n",
    "# Create a copy of the dataframe, so the original is still available for other models in the notebook\n",
    "df_logreg = df_reduced\n",
    "df_logreg = df_logreg.fillna(value=0)\n",
    "\n",
    "#... setup x, y\n",
    "if 'health' in df_logreg:\n",
    "    y = df_logreg['health'].values # get the labels we want\n",
    "    del df_logreg['health'] # get rid of the class label\n",
    "\n",
    "X = df_logreg.values # use everything else to predict!\n",
    "\n",
    "# do the cross validation\n",
    "num_cv_iterations = 10\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n=num_instances,\n",
    "                         n_iter=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "print (cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "accuracy 0.868105049458\n",
      "confusion matrix\n",
      " [[ 2283  4750]\n",
      " [ 1357 37912]]\n",
      "\n",
      "Average accuracy:  0.868105049458\n",
      "====Iteration 1  ====\n",
      "accuracy 0.865578160771\n",
      "confusion matrix\n",
      " [[ 2302  4903]\n",
      " [ 1321 37776]]\n",
      "\n",
      "Average accuracy:  0.866841605114\n",
      "====Iteration 2  ====\n",
      "accuracy 0.867802686709\n",
      "confusion matrix\n",
      " [[ 2351  4781]\n",
      " [ 1340 37830]]\n",
      "\n",
      "Average accuracy:  0.867161965646\n",
      "====Iteration 3  ====\n",
      "accuracy 0.867565115978\n",
      "confusion matrix\n",
      " [[ 2261  4806]\n",
      " [ 1326 37909]]\n",
      "\n",
      "Average accuracy:  0.867262753229\n",
      "====Iteration 4  ====\n",
      "accuracy 0.864951837934\n",
      "confusion matrix\n",
      " [[ 2245  4829]\n",
      " [ 1424 37804]]\n",
      "\n",
      "Average accuracy:  0.86680057017\n",
      "====Iteration 5  ====\n",
      "accuracy 0.867565115978\n",
      "confusion matrix\n",
      " [[ 2309  4720]\n",
      " [ 1412 37861]]\n",
      "\n",
      "Average accuracy:  0.866927994471\n",
      "====Iteration 6  ====\n",
      "accuracy 0.864152736383\n",
      "confusion matrix\n",
      " [[ 2284  4852]\n",
      " [ 1438 37728]]\n",
      "\n",
      "Average accuracy:  0.86653152903\n",
      "====Iteration 7  ====\n",
      "accuracy 0.862813701352\n",
      "confusion matrix\n",
      " [[ 2331  4977]\n",
      " [ 1375 37619]]\n",
      "\n",
      "Average accuracy:  0.86606680057\n",
      "====Iteration 8  ====\n",
      "accuracy 0.863591205563\n",
      "confusion matrix\n",
      " [[ 2303  4848]\n",
      " [ 1468 37683]]\n",
      "\n",
      "Average accuracy:  0.865791734458\n",
      "====Iteration 9  ====\n",
      "accuracy 0.866074899572\n",
      "confusion matrix\n",
      " [[ 2376  4850]\n",
      " [ 1351 37725]]\n",
      "\n",
      "Average accuracy:  0.86582005097\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "import datetime\n",
    "\n",
    "lr_clf = LogisticRegression(penalty='l2', C=1.0, class_weight=None) # get object\n",
    "\n",
    "iter_num = 0\n",
    "accuracy = 0\n",
    "\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object: \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "\n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test) # get test set predictions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print (\"====Iteration\",iter_num,\" ====\")\n",
    "    print (\"accuracy\", acc)\n",
    "    print (\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1\n",
    "    accuracy = accuracy + acc\n",
    "\n",
    "    print ('\\nAverage accuracy: ', accuracy/iter_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling and Evaluation 1\n",
    "\n",
    "We choose classification accuracy because our model is intended to classify whether someone will self-report good or better or poor or worse health quality. The overall model accuracy may not be enough, we should also examine and evaluate the confusion matrix to better understand the ability of the model to classify correctly.\n",
    "\n",
    "The first confusion matrix provides the following results:\n",
    "•\tTrue positives: 2,283\n",
    "•\tFalse positives: 4,750\n",
    "•\tFalse negatives: 1,357\n",
    "•\tTrue negative: 37,912\n",
    "\n",
    "With a large class imbalance to true negatives, it’s important to continue to look at measures like precision, recall and ultimately the F1 score to understand the effectiveness of the model. \n",
    "\n",
    "The precision for the model is the number of true positives divided by the number of true positives and false positives. The result of this calculation, also known as the positive predictive value, is .325. A large number of positive results from this categorization effort are false positives. Given that asking someone a few questions is not expensive and is easy to do, this may not be an issue.\n",
    "\n",
    "The recall value is the number of true positives divided by the number of true positives and the number of false negatives. The result of this calculation, also known as the true positive rate, is .627. \n",
    "\n",
    "If we go one step further to try to understand the balance between precision and recall, to calculate the F1 score, the result is .428. To evaluate whether this is good or not, we can estimate a cost-benefit impact based on the context of the situation. If the intent of this model was to focus public policy efforts around those variables that most impacted whether someone was going to report good or better health quality or fair or worse, our model sufficiently informs the decisions and the benefits outweigh the costs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling and Evaluation 2\n",
    "\n",
    "We want to try different penalties to see how they affect the accuracy.  We will do this by looping through\n",
    "different penalties, and comparing the accuracy in each confusion matrix to see which does better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Now testing penalty:\n",
      "====Iteration 0  ====\n",
      "accuracy 0.84842987344\n",
      "confusion matrix\n",
      " [[    0  7018]\n",
      " [    0 39284]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.846658891625\n",
      "confusion matrix\n",
      " [[    0  7100]\n",
      " [    0 39202]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.846075763466\n",
      "confusion matrix\n",
      " [[    0  7127]\n",
      " [    0 39175]]\n",
      "====Iteration 3  ====\n",
      "accuracy 0.848667444171\n",
      "confusion matrix\n",
      " [[    0  7007]\n",
      " [    0 39295]]\n",
      "====Iteration 4  ====\n",
      "accuracy 0.847177227766\n",
      "confusion matrix\n",
      " [[    0  7076]\n",
      " [    0 39226]]\n",
      "====Iteration 5  ====\n",
      "accuracy 0.849120988294\n",
      "confusion matrix\n",
      " [[    0  6986]\n",
      " [    0 39316]]\n",
      "====Iteration 6  ====\n",
      "accuracy 0.847155630426\n",
      "confusion matrix\n",
      " [[    0  7077]\n",
      " [    0 39225]]\n",
      "====Iteration 7  ====\n",
      "accuracy 0.846831670338\n",
      "confusion matrix\n",
      " [[    0  7092]\n",
      " [    0 39210]]\n",
      "====Iteration 8  ====\n",
      "accuracy 0.845039091184\n",
      "confusion matrix\n",
      " [[    0  7175]\n",
      " [    0 39127]]\n",
      "====Iteration 9  ====\n",
      "accuracy 0.844499157704\n",
      "confusion matrix\n",
      " [[    0  7200]\n",
      " [    0 39102]]\n",
      "\n",
      "Average accuracy:  0.846965573841\n",
      "\n",
      "\n",
      "Now testing penalty:\n",
      "====Iteration 0  ====\n",
      "accuracy 0.845643816682\n",
      "confusion matrix\n",
      " [[    0  7147]\n",
      " [    0 39155]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.846658891625\n",
      "confusion matrix\n",
      " [[    0  7100]\n",
      " [    0 39202]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.846918059695\n",
      "confusion matrix\n",
      " [[    0  7088]\n",
      " [    0 39214]]\n",
      "====Iteration 3  ====\n",
      "accuracy 0.84596777677\n",
      "confusion matrix\n",
      " [[    0  7132]\n",
      " [    0 39170]]\n",
      "====Iteration 4  ====\n",
      "accuracy 0.844110405598\n",
      "confusion matrix\n",
      " [[    0  7218]\n",
      " [    0 39084]]\n",
      "====Iteration 5  ====\n",
      "accuracy 0.845751803378\n",
      "confusion matrix\n",
      " [[    0  7142]\n",
      " [    0 39160]]\n",
      "====Iteration 6  ====\n",
      "accuracy 0.847717161246\n",
      "confusion matrix\n",
      " [[    0  7051]\n",
      " [    0 39251]]\n",
      "====Iteration 7  ====\n",
      "accuracy 0.846723683642\n",
      "confusion matrix\n",
      " [[    0  7097]\n",
      " [    0 39205]]\n",
      "====Iteration 8  ====\n",
      "accuracy 0.845017493845\n",
      "confusion matrix\n",
      " [[    0  7176]\n",
      " [    0 39126]]\n",
      "====Iteration 9  ====\n",
      "accuracy 0.846702086303\n",
      "confusion matrix\n",
      " [[    0  7098]\n",
      " [    0 39204]]\n",
      "\n",
      "Average accuracy:  0.846121117878\n"
     ]
    }
   ],
   "source": [
    "penalties = ('l1', 'l2')\n",
    "for p in penalties:\n",
    "    print ('\\n\\nNow testing penalty:'), p\n",
    "    \n",
    "    lr_clf = LogisticRegression(penalty=p, C=1.0, class_weight=None) # get object\n",
    "\n",
    "    iter_num = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    # the indices are the rows used for training and testing in each iteration\n",
    "    for train_indices, test_indices in cv_object: \n",
    "        X_train = X[train_indices]\n",
    "        y_train = y[train_indices]\n",
    "\n",
    "        X_test = X[test_indices]\n",
    "        y_test = y[test_indices]\n",
    "\n",
    "        # train the reusable logisitc regression model on the training data\n",
    "        lr_clf.fit(X_train,y_train)  # train object\n",
    "        y_hat = lr_clf.predict(X_test) # get test set predictions\n",
    "\n",
    "        # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "        acc = mt.accuracy_score(y_test,y_hat)\n",
    "        conf = mt.confusion_matrix(y_test,y_hat)\n",
    "        print (\"====Iteration\",iter_num,\" ====\")\n",
    "        print (\"accuracy\", acc)\n",
    "        print (\"confusion matrix\\n\",conf)\n",
    "        iter_num+=1\n",
    "        accuracy = accuracy + acc\n",
    "\n",
    "    print ('\\nAverage accuracy: ', accuracy/iter_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import k-fold cross validation from scikit learn\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "if 'health' in df_logreg:\n",
    "    y = df_logreg['health'].values # get the labels we want\n",
    "    del df_logreg['health'] # get ride of the class label\n",
    "    X = df_logreg.values # use everything else to predict!\n",
    "    \n",
    "    \n",
    "KFoldCrossObject = KFold(len(y), n_folds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8486825054\n",
      "[[ 1337  2760]\n",
      " [  743 18310]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'zip' object has no attribute 'sort'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-f9d0375331c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# sort these attributes and spit them out\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mzip_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogReg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf_logreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# combine attributes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mzip_vars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# sort them by the magnitude of the weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcoef\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip_vars\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'has weight of'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoef\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# now print them out\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'zip' object has no attribute 'sort'"
     ]
    }
   ],
   "source": [
    "for train_indices, test_indices in KFoldCrossObject: \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "\n",
    "# scale attributes by the training set\n",
    "scale = StandardScaler()\n",
    "scale.fit(X_train) # find scalings for each column that make this zero mean and unit std\n",
    "\n",
    "X_train_scaled = scale.transform(X_train) # apply to training\n",
    "X_test_scaled = scale.transform(X_test) # apply those means and std to the test set (without snooping at the test set values)\n",
    "\n",
    "# train the model just as before\n",
    "logReg = LogisticRegression(penalty='l2', C=0.05, n_jobs=-1) \n",
    "logReg.fit(X_train_scaled,y_train)  # train object\n",
    "\n",
    "y_hat = logReg.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf )\n",
    "\n",
    "# sort these attributes and spit them out\n",
    "zip_vars = zip(logReg.coef_.T,df_logreg.columns) # combine attributes\n",
    "zip_vars.sort(key = lambda t: np.abs(t[0])) # sort them by the magnitude of the weight\n",
    "for coef, name in zip_vars:\n",
    "    print(name, 'has weight of', coef[0]) # now print them out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuract here is 84.9%, which is a little worse than above, but our cross validation step was added here.\n",
    "Looking at our data, we can see the weights of the different variables in the text above, or graphed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA24AAAEvCAYAAAA9ypKHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVLj/ARAyMpmZCbD5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa7fcbe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[15,4])\n",
    "\n",
    "weights = pd.Series(logReg.coef_[0],index=df_logreg.columns)\n",
    "weights.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Regression\n",
    "\n",
    "We've identified the relative weighting of the variables and their impact on self-reported health quality, but what if we could narrow it down from 9 variables to one? We'll pursue gradient boosting regression to understand if one of the variables could serve as a proxy for all others.\n",
    "\n",
    "In looking at the graphic above, it looks like the amount of exercise someone gets has the biggest impact on quality of health. So we’ll select that as the explanatory variable and keep self-reported health quality as our response variable. We’ll set the number of regression trees to 500, the depth of each individual tree to 4 the loss function to least squares and the learning rate to .01. The resulting mean squared error is .0859, which indicates a lack of fit between self-reported health quality and physical health. It would appear as though we need more variables in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.0859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0xbbd2828>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAGJCAYAAACThGjuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xbbdc1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# code built from: \n",
    "# http://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regression.html#example-ensemble-plot-gradient-boosting-regression-py\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "###############################################################################\n",
    "# Load data\n",
    "X, y = shuffle(df_reduced1.PHYSHLTH.reshape(-1,1), df_reduced1.health.ravel(), random_state=13)\n",
    "offset = int(X.shape[0] * 0.9)\n",
    "X_train, y_train = X[:offset], y[:offset]\n",
    "X_test, y_test = X[offset:], y[offset:]\n",
    "\n",
    "###############################################################################\n",
    "# Fit regression model\n",
    "params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 1,\n",
    "          'learning_rate': 0.01, 'loss': 'ls'}\n",
    "clf = ensemble.GradientBoostingRegressor(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "mse = mean_squared_error(y_test, clf.predict(X_test))\n",
    "print(\"MSE: %.4f\" % mse)\n",
    "\n",
    "###############################################################################\n",
    "# Plot training deviance\n",
    "\n",
    "# compute test set deviance\n",
    "test_score = np.zeros((params['n_estimators'],), dtype=np.float64)\n",
    "\n",
    "for i, y_pred in enumerate(clf.staged_predict(X_test)):\n",
    "    test_score[i] = clf.loss_(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Deviance')\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, clf.train_score_, 'b-',\n",
    "         label='Training Set Deviance')\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-',\n",
    "         label='Test Set Deviance')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Boosting Iterations')\n",
    "plt.ylabel('Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Chart Above\n",
    "\n",
    "The chart above shows a very small difference between our training and test data set, which is a good thing, but the very low MSE means a lack of fit between the variables and an overall weak predictive model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# K-Nearest Neighbors\n",
    "---\n",
    "\n",
    "In this section, will do an prediction analysis based on k-nearest neighbors (KNN).\n",
    "This should give us clusters based on how well each point clusters on others.\n",
    "\n",
    "We will use 10-fold cross validation, and 10 nearest neighbors. For the neighbors, 10 is an arbitrary value, chosed because we have a really big dataset.  ** WE SHOULD VARY THIS TO SEE WHAT DIFFERENCE IT MAKES **\n",
    "We also use 10-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  1.,   3.,  88., ...,   2.,   1.,   1.],\n",
       "        [  1.,   3.,   2., ...,   2.,   1.,   1.],\n",
       "        [  1.,   4.,   3., ...,   2.,   1.,   1.],\n",
       "        ..., \n",
       "        [  0.,   3.,  20., ...,   2.,   1.,   5.],\n",
       "        [  1.,   4.,  88., ...,   2.,   1.,   5.],\n",
       "        [  1.,   4.,  88., ...,   2.,   1.,   5.]]),\n",
       " array([ 1.,  1.,  1., ...,  0.,  1.,  1.]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial setup\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "knn = df_reduced  # Copy the dataset for this analysis.\n",
    "knn.fillna(value=0)\n",
    "\n",
    "# and setup our X and Y\n",
    "if '_Health' in knn:\n",
    "    y = knn['_Health'].values # get the labels we want\n",
    "    del knn['_Health'] # get rid of the class label\n",
    "\n",
    "yhat = np.zeros(y.shape)  #empty array to fill with predictions m\n",
    "X = knn.values\n",
    "\n",
    "# Display the arrays.  Not really needed, but tells us that there are some nan values, which break things.\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data Cleanup. In conversion to numpy array from pandas\n",
    "# some values are being converted to nan or inf values.\n",
    "X=np.nan_to_num(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fit the KNN model to our data\n",
    "cv = StratifiedKFold(y, n_folds=10)\n",
    "\n",
    "# get a handle to the classifier object, which defines the type\n",
    "clf = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "for train, test in cv:\n",
    "    clf.fit(X[train],y[train])\n",
    "    yhat[test] = clf.predict(X[test])\n",
    "\n",
    "total_accuracy = mt.accuracy_score(y, yhat)\n",
    "print 'KNN accuracy', total_accuracy\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# intentionally left blank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Pipeline with PCA and KNN\n",
    "---\n",
    "\n",
    "Wow, the KNN got 87.7% accuracy. That's really good. But is it accurate? Now trying a pipeline doing a PCA first, then the KNN again. (Again, with k=10.)\n",
    "Expect it will get even better, since PCA should eliminate whats no good. However, at this point we need to watch for overfitting. if our accuracy jumps real high, we should be suspicious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import RandomizedPCA \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# setup pipeline to take PCA, then fit a KNN classifier\n",
    "clf_pipe = Pipeline(\n",
    "    [('PCA',RandomizedPCA(n_components=300)),\n",
    "     ('CLF',KNeighborsClassifier(n_neighbors=10))]\n",
    ")\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "for train, test in cv:\n",
    "    clf_pipe.fit(X[train],y[train])\n",
    "    yhat[test] = clf_pipe.predict(X[test])\n",
    "\n",
    "total_accuracy = mt.accuracy_score(y, yhat)\n",
    "print 'KNN, pipeline accuracy', total_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's the same.  That's cool, since it's still using KNN.\n",
    "Let's try something else..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Random Forest Classifier\n",
    "--- \n",
    "Since two different versions of KNN came out the same, lets compare it to something else.  We'll try random forest in our pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_pipe = Pipeline(\n",
    "    [('PCA',RandomizedPCA(n_components=100)),\n",
    "     ('CLF',RandomForestClassifier(max_depth=50, n_estimators=150, n_jobs=-1))]\n",
    ")\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "for train, test in cv:\n",
    "    clf_pipe.fit(X[train],y[train])\n",
    "    yhat[test] = clf_pipe.predict(X[test])\n",
    "    \n",
    "total_accuracy = mt.accuracy_score(y, yhat)\n",
    "print 'Pipeline accuracy %0.2f (+/- %0.2f)' % total_accuracy % (total_accuracy.std()*2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very suspicious because it's so high. Seems likely we are overfitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Deployment\n",
    "\n",
    "For individuals, the model helps to clarify the relationship between self-reported health quality and a number of demographics and behaviors. The demographics are obviously beyond a person’s control, but the behaviors are not. The model shows that education level has the strongest correlation to good or better self-reported health quality, which we didn’t anticipate. Even if we can’t go so far as to interpret a cause and effect relationship, it appears as though those with more education self-report higher health quality. \n",
    "\n",
    "For policy makers, there is a constant competition for available public funds. If the case can be made that education not only helps a person to be more successful in their life, but also has a strong correlation to health quality, it could better make the case for increased investment. \n",
    "\n",
    "Many of these variables change over time and since the survey is conducted on an annual basis, it would be important to update the model each year and observe any changes between the relative weighting of the variables in the model. \n",
    "\n",
    "If the intent of the survey, the instrument where all this data came from, was to predict health quality, the surveyors could theoretically shorten the survey from 200+ variables to these key 9 and achieve similar results, at a much lower cost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
