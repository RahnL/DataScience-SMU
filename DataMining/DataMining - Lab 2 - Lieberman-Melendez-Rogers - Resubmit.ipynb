{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Data Mining - MSDS 7331 - Thurs 6:30, Summer 2016\n",
    "\n",
    "Team 3 (AKA Team Super Awesome):  Sal Melendez, Rahn Lieberman, Thomas Rogers\n",
    "\n",
    "Github page:\n",
    "https://github.com/RahnL/DataScience-SMU/tree/master/DataMining\n",
    "\n",
    "Note: Code borrowed heavily from Eric Larson's github pages for this class.\n",
    "https://github.com/eclarson/DataMiningNotebooks/blob/master/04.%20Logits%20and%20SVM.ipynb\n",
    "\n",
    "Code also borrowed from other projects we're working on using the same dataset.\n",
    "\n",
    "https://github.com/RahnL/DataScience-SMU/blob/master/DataMining/DataMining-MiniLab1-Lieberman-Melendez-Rogers.ipynb\n",
    "https://github.com/rlshuhart/MSDS6210-Immersion_Project/blob/master/Study/Closing%20the%20Gap%20Study%20Revisited.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "* Intro\n",
    "* Data Cleanup and Reduction\n",
    "\n",
    "**Classifications:**\n",
    "* Logistical Regression\n",
    "* K-Nearest Neighbor, with and without PCA\n",
    "* Random Forest\n",
    "* Gradient Boosting Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "\n",
    "Our team has selected the 2014 Behavioral Risk Factor Surveillance System data (BRFSS), from the Center for Disease Control and prevention (CDC), to attempt to understand the relationship between quality of health and a number of behavioral, demographic and environmental factors. \n",
    "\n",
    "The purpose of the BRFSS project is to survey a large population of Americans on a wide range of topics to inform policy, research and healthcare delivery. The same or similar questions are asked each year and the resulting dataset gives not only a broad, comprehensive view of health quality in the United States, but it also provides a longitudinal view on how quality of care (among other factors) is changing over time.\n",
    "\n",
    "There are 279 variables in the dataset and over 460,000 surveys completed. The sheer breadth and complexity of this data, with missing, weighted and calculated variables requires a clear and distinct question of interest and some sense of what variables might help answer the question. We have chosen to focus on one particular question in the survey as our response variable and will attempt to better understand the impact reported behaviors have on responses to that question. \n",
    "\n",
    "Our response variable becomes the answer to the following question on quality of health: \"Would you say that in general your health is: (1) excellent, (2) very good, (3) good, (4) fair, (5) poor?\" (section 1.1, column 80)\n",
    "\n",
    "We reduce the 279 variables to focus on those related to behavioral survey questions. <span style=\"color:brown\">That is, after manually evaluating all the variables, we kept demographic data, and questions that directly related to the health of participants. Variables what were follow-up to previous questions were taken out. For example, \"do you smoke?\" was kept in, but \"how often do you smoke?\" was taken out, since it's not relevant to non-smokers</span>. The corresponding variables from the questions related to behavior number 30, so our dataset is roughly 450,000 rows by 30 columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# plot graphs in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting length is 464664 \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 464664 entries, 0 to 464663\n",
      "Columns: 279 entries, _STATE to RCSBIRTH\n",
      "dtypes: float64(226), int64(52), object(1)\n",
      "memory usage: 989.1+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_STATE</th>\n",
       "      <th>FMONTH</th>\n",
       "      <th>IDATE</th>\n",
       "      <th>IMONTH</th>\n",
       "      <th>IDAY</th>\n",
       "      <th>IYEAR</th>\n",
       "      <th>DISPCODE</th>\n",
       "      <th>SEQNO</th>\n",
       "      <th>_PSU</th>\n",
       "      <th>CTELENUM</th>\n",
       "      <th>...</th>\n",
       "      <th>_FOBTFS</th>\n",
       "      <th>_CRCREC</th>\n",
       "      <th>_AIDTST3</th>\n",
       "      <th>_IMPEDUC</th>\n",
       "      <th>_IMPMRTL</th>\n",
       "      <th>_IMPHOME</th>\n",
       "      <th>RCSBRAC1</th>\n",
       "      <th>RCSRACE1</th>\n",
       "      <th>RCHISLA1</th>\n",
       "      <th>RCSBIRTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1172014</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>2014</td>\n",
       "      <td>1100</td>\n",
       "      <td>2014000001</td>\n",
       "      <td>2014000001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1072014</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2014</td>\n",
       "      <td>1100</td>\n",
       "      <td>2014000002</td>\n",
       "      <td>2014000002</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1092014</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2014</td>\n",
       "      <td>1100</td>\n",
       "      <td>2014000003</td>\n",
       "      <td>2014000003</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1072014</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2014</td>\n",
       "      <td>1100</td>\n",
       "      <td>2014000004</td>\n",
       "      <td>2014000004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1162014</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>2014</td>\n",
       "      <td>1100</td>\n",
       "      <td>2014000005</td>\n",
       "      <td>2014000005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 279 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   _STATE  FMONTH    IDATE  IMONTH  IDAY  IYEAR  DISPCODE       SEQNO  \\\n",
       "0       1       1  1172014       1    17   2014      1100  2014000001   \n",
       "1       1       1  1072014       1     7   2014      1100  2014000002   \n",
       "2       1       1  1092014       1     9   2014      1100  2014000003   \n",
       "3       1       1  1072014       1     7   2014      1100  2014000004   \n",
       "4       1       1  1162014       1    16   2014      1100  2014000005   \n",
       "\n",
       "         _PSU  CTELENUM    ...     _FOBTFS  _CRCREC  _AIDTST3  _IMPEDUC  \\\n",
       "0  2014000001       1.0    ...         2.0      1.0       2.0         5   \n",
       "1  2014000002       1.0    ...         2.0      2.0       2.0         4   \n",
       "2  2014000003       1.0    ...         2.0      2.0       2.0         6   \n",
       "3  2014000004       1.0    ...         2.0      1.0       2.0         6   \n",
       "4  2014000005       1.0    ...         2.0      1.0       2.0         5   \n",
       "\n",
       "   _IMPMRTL  _IMPHOME  RCSBRAC1  RCSRACE1  RCHISLA1  RCSBIRTH  \n",
       "0         1         1       NaN       NaN       NaN       NaN  \n",
       "1         1         1       NaN       NaN       NaN       NaN  \n",
       "2         1         1       NaN       NaN       NaN       NaN  \n",
       "3         3         1       NaN       NaN       NaN       NaN  \n",
       "4         1         1       NaN       NaN       NaN       NaN  \n",
       "\n",
       "[5 rows x 279 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the dataset, and do some initial cleanup.\n",
    "\n",
    "df = pd.read_csv(\"data/LLCP2014XPT.txt\", sep=\"\\t\", encoding = \"ISO-8859-1\")\n",
    "df.head()\n",
    "\n",
    "print(\"Starting length is %.f \" % len(df))\n",
    "\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reduction and Pre-processing\n",
    "\n",
    "We're interested in the relationship between behaviors, demographics and other factors, and the impact they have on general health quality. For this reason, we'll narrow down the dataset to those variables that we believe are most pertinent. Smoking and the level of physical activity have a demonstrated impact on health quality and they’re behaviors. Age has an obvious impact, but education, income level and race may have a less obvious impact. The cost and access to care have been identified as factors that negatively impact health, so we’ll include those as well. Our model ultimately will show the relative impact these variables have on health quality. \n",
    "\n",
    "#### Behaviors:\n",
    "- Whether someone smokes or not (represented by _SMOKER3)\n",
    "- Physical activity (represented by PHYSHLTH)\n",
    "\n",
    "#### Demographics:\n",
    "- Age (represented by _AGE_G)\n",
    "- Education level (represented by EDUCA)\n",
    "- Income level (represented by _INCOMG)\n",
    "- Race (represented by _IMPRACE, an imputed value based on the initial data ste)\n",
    "\n",
    "#### Other Factors:\n",
    "- The cost of health care (represented by MEDCOST)\n",
    "- Health coverage (represented by HLTHPLN1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Data Preparation Part 2\n",
    "<span style=\"color:brown\">\n",
    "After analyzing the distribution of a number of variables, the age variable is contains a large percentage of people that are over 65. Fearing the skewness this will have on our data, we narrow the dataset to only contain those 18 to 64. \n",
    "</span>\n",
    "\n",
    "<span style=\"color:brown\">\n",
    "Related to our variable of interest, we removed those responses where the answer provided was “don’t know”, “not sure” or “refused”. We did the same for the variable “HLTHPLN1”. Because they were such small percentage of the population, we imputed a variable for race, by removing those who identified as Asian, American Indian/Alaskan Native or Other. \n",
    "</span>\n",
    "\n",
    "<span style=\"color:brown\">\n",
    "In order to allow for logistic regression, we translated our variable of interest from five variables “excellent”, “very good”, “good”, “fair” and “poor” to a binary variable, “good or better” and “fair or worse”.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Age 18 to 64 - Excludes 65 or older, refused, or missing\n",
    "df = df[df['_AGE65YR'] == 1].drop('_AGE65YR', axis=1)\n",
    "\n",
    "# Exclude blank, 'Don't know', 'Not Sure', or 'Refused'\n",
    "df = df[((df['GENHLTH'].notnull()) & (~df['GENHLTH'].isin([7,9])))] \n",
    "\n",
    "# Reduce Ethnicity to White, Black, or Hispanic (ex. Asian 2%, American Indian/Alaskan Native 1.55%, other 2.8%)\n",
    "df = df[df['_IMPRACE'].isin([1,2,5])]\n",
    "# Has Health plan --Excludes 'Don't know', 'Not Sure', or 'Refused'. drops .6%\n",
    "df = df[df['HLTHPLN1'].isin([1,2])]\n",
    "\n",
    "# Translate GENHLTH to binary classification of\n",
    "# Combining the “excellent”, “very good” and “good” responses as measures of “good or better” (1) health \n",
    "# and the “fair” and “poor” measures as “fair and poor” (0).\n",
    "df.loc[(df['GENHLTH'] < 4), 'health'] = 1\n",
    "df.loc[(df['GENHLTH'] >= 4), 'health'] = 0\n",
    "\n",
    "# Extract survey year from sequence. IYEAR sometimes went into the next year. \n",
    "# This is one way to put designate the year of the data publication\n",
    "# Also, if we add  other years to the data, this seperates it.\n",
    "df['Rec_Year'] = df['SEQNO'].astype(str).str[:4].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 231507 entries, 2 to 464663\n",
      "Data columns (total 9 columns):\n",
      "health      231507 non-null float64\n",
      "_SMOKER3    231507 non-null float64\n",
      "PHYSHLTH    231507 non-null float64\n",
      "_AGE_G      231507 non-null int64\n",
      "EDUCA       231507 non-null float64\n",
      "_INCOMG     231507 non-null float64\n",
      "MEDCOST     231507 non-null float64\n",
      "HLTHPLN1    231507 non-null int64\n",
      "_IMPRACE    231507 non-null int64\n",
      "dtypes: float64(6), int64(3)\n",
      "memory usage: 17.7 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahnl\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\rahnl\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>health</th>\n",
       "      <th>_SMOKER3</th>\n",
       "      <th>PHYSHLTH</th>\n",
       "      <th>_AGE_G</th>\n",
       "      <th>EDUCA</th>\n",
       "      <th>_INCOMG</th>\n",
       "      <th>MEDCOST</th>\n",
       "      <th>HLTHPLN1</th>\n",
       "      <th>_IMPRACE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    health  _SMOKER3  PHYSHLTH  _AGE_G  EDUCA  _INCOMG  MEDCOST  HLTHPLN1  \\\n",
       "2      1.0       3.0      88.0       4    6.0      5.0      2.0         1   \n",
       "6      1.0       3.0       2.0       5    6.0      5.0      2.0         1   \n",
       "7      1.0       4.0       3.0       5    4.0      1.0      2.0         1   \n",
       "9      1.0       2.0       1.0       3    5.0      5.0      2.0         1   \n",
       "12     1.0       4.0      88.0       3    6.0      5.0      2.0         1   \n",
       "13     1.0       3.0      88.0       5    6.0      3.0      2.0         1   \n",
       "22     0.0       4.0      22.0       4    5.0      2.0      1.0         1   \n",
       "24     1.0       1.0      88.0       2    5.0      1.0      1.0         1   \n",
       "25     0.0       1.0      30.0       3    2.0      2.0      1.0         2   \n",
       "26     1.0       4.0      88.0       5    5.0      5.0      2.0         1   \n",
       "27     1.0       4.0       1.0       5    5.0      5.0      2.0         1   \n",
       "31     1.0       4.0      88.0       4    5.0      5.0      2.0         1   \n",
       "38     1.0       4.0      88.0       4    5.0      5.0      2.0         1   \n",
       "39     1.0       4.0      88.0       5    4.0      4.0      2.0         1   \n",
       "41     1.0       4.0      88.0       4    5.0      3.0      2.0         1   \n",
       "43     0.0       3.0      88.0       5    6.0      2.0      2.0         1   \n",
       "45     0.0       1.0      30.0       5    4.0      2.0      1.0         1   \n",
       "47     1.0       4.0      88.0       5    5.0      2.0      2.0         1   \n",
       "48     1.0       4.0      88.0       4    5.0      5.0      2.0         1   \n",
       "53     1.0       4.0      88.0       5    3.0      2.0      2.0         2   \n",
       "\n",
       "    _IMPRACE  \n",
       "2          1  \n",
       "6          1  \n",
       "7          1  \n",
       "9          1  \n",
       "12         1  \n",
       "13         1  \n",
       "22         1  \n",
       "24         1  \n",
       "25         1  \n",
       "26         1  \n",
       "27         1  \n",
       "31         1  \n",
       "38         1  \n",
       "39         1  \n",
       "41         1  \n",
       "43         1  \n",
       "45         1  \n",
       "47         1  \n",
       "48         1  \n",
       "53         1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the variables we want to look at to a new DF, and a little more cleanup\n",
    "df_reduced = df[['health','_SMOKER3','PHYSHLTH','_AGE_G','EDUCA','_INCOMG','MEDCOST','HLTHPLN1','_IMPRACE']]\n",
    "\n",
    "# Cleanup\n",
    "df_reduced.replace(7,np.nan, inplace=True)  #replace the \"refused\" answer choice\n",
    "df_reduced.replace(9, np.nan, inplace=True) #replace the 'Don't Know' choice\n",
    "df_reduced = df_reduced.dropna() # this drops those that were the refused/don't know.\n",
    "\n",
    "df_reduced.info()\n",
    "df_reduced.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='health', dtype='float64',\n",
       "       handle_unknown='error', n_values='auto', sparse=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = OneHotEncoder()\n",
    "enc.fit(df_reduced) \n",
    "OneHotEncoder(categorical_features='health', dtype='float64', handle_unknown='error', n_values='auto', sparse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>health</th>\n",
       "      <th>_SMOKER3</th>\n",
       "      <th>PHYSHLTH</th>\n",
       "      <th>_AGE_G</th>\n",
       "      <th>EDUCA</th>\n",
       "      <th>_INCOMG</th>\n",
       "      <th>MEDCOST</th>\n",
       "      <th>HLTHPLN1</th>\n",
       "      <th>_IMPRACE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    health  _SMOKER3  PHYSHLTH  _AGE_G  EDUCA  _INCOMG  MEDCOST  HLTHPLN1  \\\n",
       "2      1.0       3.0      88.0       4    6.0      5.0      2.0         1   \n",
       "6      1.0       3.0       2.0       5    6.0      5.0      2.0         1   \n",
       "7      1.0       4.0       3.0       5    4.0      1.0      2.0         1   \n",
       "9      1.0       2.0       1.0       3    5.0      5.0      2.0         1   \n",
       "12     1.0       4.0      88.0       3    6.0      5.0      2.0         1   \n",
       "13     1.0       3.0      88.0       5    6.0      3.0      2.0         1   \n",
       "22     0.0       4.0      22.0       4    5.0      2.0      1.0         1   \n",
       "24     1.0       1.0      88.0       2    5.0      1.0      1.0         1   \n",
       "25     0.0       1.0      30.0       3    2.0      2.0      1.0         2   \n",
       "26     1.0       4.0      88.0       5    5.0      5.0      2.0         1   \n",
       "\n",
       "    _IMPRACE  \n",
       "2          1  \n",
       "6          1  \n",
       "7          1  \n",
       "9          1  \n",
       "12         1  \n",
       "13         1  \n",
       "22         1  \n",
       "24         1  \n",
       "25         1  \n",
       "26         1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reduced.head(10)\n",
    "df_reduced1 = df_reduced\n",
    "df_reduced1.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---\n",
    "# Logistic Regression\n",
    "---\n",
    "\n",
    "Here, we are performing a logistic regression test to see how accurately we can predict health based on our chosen variables.\n",
    "\n",
    "We perform a 10-fold cross validation, using an 80/20 split for training and testing. <span style=\"color:brown\">We do 10 folds in order to see if there are significant differences in the accuracy over a large number of folds and if the extra time involved in added extra folds is worth the effort. We can extract the 3-fold and 5-fold averages from this. \n",
    "</span>\n",
    "\n",
    "<span style=\"color:brown\">From a business perspective, if there is only a small gain, say less than 0.1%, then it may not be worth doing 10 folds in in long term, since the smaller fold number will get us nearly equivalent results. (And we could perform a T-test to determine if the differences is significant.)</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(231507, n_iter=10, test_size=0.2, random_state=None)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "\n",
    "# rerun with all variables, filling NaN values with zero\n",
    "# Create a copy of the dataframe, so the original is still available for other models in the notebook\n",
    "df_logreg = df_reduced\n",
    "df_logreg = df_logreg.fillna(value=0)\n",
    "\n",
    "#... setup x, y\n",
    "if 'health' in df_logreg:\n",
    "    y = df_logreg['health'].values # get the labels we want\n",
    "    del df_logreg['health'] # get rid of the class label\n",
    "\n",
    "X = df_logreg.values # use everything else to predict!\n",
    "\n",
    "# do the cross validation\n",
    "num_cv_iterations = 10\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n=num_instances,\n",
    "                         n_iter=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "\n",
    "print (cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('====Iteration', 0, ' ====')\n",
      "('accuracy', 0.86609649691158053)\n",
      "('confusion matrix\\n', array([[ 2188,  4876],\n",
      "       [ 1324, 37914]]))\n",
      "('\\nAverage accuracy: ', 0.86609649691158053)\n",
      "('====Iteration', 1, ' ====')\n",
      "('accuracy', 0.8672627532288022)\n",
      "('confusion matrix\\n', array([[ 2294,  4819],\n",
      "       [ 1327, 37862]]))\n",
      "('\\nAverage accuracy: ', 0.86667962507019136)\n",
      "('====Iteration', 2, ' ====')\n",
      "('accuracy', 0.86324564813614968)\n",
      "('confusion matrix\\n', array([[ 2363,  4917],\n",
      "       [ 1415, 37607]]))\n",
      "('\\nAverage accuracy: ', 0.8655349660921775)\n",
      "('====Iteration', 3, ' ====')\n",
      "('accuracy', 0.86678761176623043)\n",
      "('confusion matrix\\n', array([[ 2296,  4848],\n",
      "       [ 1320, 37838]]))\n",
      "('\\nAverage accuracy: ', 0.86584812751069073)\n",
      "('====Iteration', 4, ' ====')\n",
      "('accuracy', 0.86443350179257916)\n",
      "('confusion matrix\\n', array([[ 2335,  4942],\n",
      "       [ 1335, 37690]]))\n",
      "('\\nAverage accuracy: ', 0.86556520236706846)\n",
      "('====Iteration', 5, ' ====')\n",
      "('accuracy', 0.86579413416267115)\n",
      "('confusion matrix\\n', array([[ 2321,  4892],\n",
      "       [ 1322, 37767]]))\n",
      "('\\nAverage accuracy: ', 0.86560335766633567)\n",
      "('====Iteration', 6, ' ====')\n",
      "('accuracy', 0.86356960822426676)\n",
      "('confusion matrix\\n', array([[ 2286,  4924],\n",
      "       [ 1393, 37699]]))\n",
      "('\\nAverage accuracy: ', 0.8653128220317543)\n",
      "('====Iteration', 7, ' ====')\n",
      "('accuracy', 0.86631247030365854)\n",
      "('confusion matrix\\n', array([[ 2281,  4847],\n",
      "       [ 1343, 37831]]))\n",
      "('\\nAverage accuracy: ', 0.8654377780657424)\n",
      "('====Iteration', 8, ' ====')\n",
      "('accuracy', 0.86648524901732105)\n",
      "('confusion matrix\\n', array([[ 2233,  4820],\n",
      "       [ 1362, 37887]]))\n",
      "('\\nAverage accuracy: ', 0.86555416372702898)\n",
      "('====Iteration', 9, ' ====')\n",
      "('accuracy', 0.86527579802168375)\n",
      "('confusion matrix\\n', array([[ 2280,  4882],\n",
      "       [ 1356, 37784]]))\n",
      "('\\nAverage accuracy: ', 0.86552632715649447)\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "import datetime\n",
    "\n",
    "lr_clf = LogisticRegression(penalty='l2', C=1.0, class_weight=None) # get object\n",
    "\n",
    "iter_num = 0\n",
    "accuracy = 0\n",
    "\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object: \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "\n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test) # get test set predictions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print (\"====Iteration\",iter_num,\" ====\")\n",
    "    print (\"accuracy\", acc)\n",
    "    print (\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1\n",
    "    accuracy = accuracy + acc\n",
    "\n",
    "    print ('\\nAverage accuracy: ', accuracy/iter_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:brown\">From this, we see the accuracy is between 0.866 and 0.868, with rounding. From this, we probably would not use the extra folds.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:brown\"> *** whole section added ***</span>\n",
    "\n",
    "### Modeling and Evaluation 1\n",
    "\n",
    "We choose classification accuracy because our model is intended to classify whether someone will self-report good or better or poor or worse health quality. The overall model accuracy may not be enough, we should also examine and evaluate the confusion matrix to better understand the ability of the model to classify correctly.\n",
    "\n",
    "The first confusion matrix provides the following results:\n",
    "*\tTrue positives: 2,283\n",
    "*\tFalse positives: 4,750\n",
    "*\tFalse negatives: 1,357\n",
    "*\tTrue negative: 37,912\n",
    "\n",
    "With a large class imbalance to true negatives, it’s important to continue to look at measures like precision, recall and ultimately the F1 score to understand the effectiveness of the model. \n",
    "\n",
    "The precision for the model is the number of true positives divided by the number of true positives and false positives. The result of this calculation, also known as the positive predictive value, is .325. A large number of positive results from this categorization effort are false positives. Given that asking someone a few questions is not expensive and is easy to do, this may not be an issue.\n",
    "\n",
    "The recall value is the number of true positives divided by the number of true positives and the number of false negatives. The result of this calculation, also known as the true positive rate, is .627. \n",
    "\n",
    "If we go one step further to try to understand the balance between precision and recall, to calculate the F1 score, the result is .428. To evaluate whether this is good or not, we can estimate a cost-benefit impact based on the context of the situation. If the intent of this model was to focus public policy efforts around those variables that most impacted whether someone was going to report good or better health quality or fair or worse, our model sufficiently informs the decisions and the benefits outweigh the costs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Modeling and Evaluation 2\n",
    "\n",
    "We want to try different penalties to see how they affect the accuracy.  We will do this by looping through\n",
    "different penalties, and comparing the accuracy in each confusion matrix to see which does better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Now testing penalty: l1\n",
      "====Iteration 0  ====\n",
      "accuracy 0.864519891149\n",
      "confusion matrix: \n",
      "[[ 2273  4935]\n",
      " [ 1338 37756]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.866204483608\n",
      "confusion matrix: \n",
      "[[ 2299  4844]\n",
      " [ 1351 37808]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.864973435273\n",
      "confusion matrix: \n",
      "[[ 2314  4853]\n",
      " [ 1399 37736]]\n",
      "====Iteration 3  ====\n",
      "accuracy 0.865945315537\n",
      "confusion matrix: \n",
      "[[ 2236  4875]\n",
      " [ 1332 37859]]\n",
      "====Iteration 4  ====\n",
      "accuracy 0.86667962507\n",
      "confusion matrix: \n",
      "[[ 2290  4807]\n",
      " [ 1366 37839]]\n",
      "====Iteration 5  ====\n",
      "accuracy 0.867089974515\n",
      "confusion matrix: \n",
      "[[ 2335  4825]\n",
      " [ 1329 37813]]\n",
      "====Iteration 6  ====\n",
      "accuracy 0.864951837934\n",
      "confusion matrix: \n",
      "[[ 2288  4891]\n",
      " [ 1362 37761]]\n",
      "====Iteration 7  ====\n",
      "accuracy 0.86505982463\n",
      "confusion matrix: \n",
      "[[ 2227  4797]\n",
      " [ 1451 37827]]\n",
      "====Iteration 8  ====\n",
      "accuracy 0.866701222409\n",
      "confusion matrix: \n",
      "[[ 2322  4788]\n",
      " [ 1384 37808]]\n",
      "====Iteration 9  ====\n",
      "accuracy 0.867867478727\n",
      "confusion matrix: \n",
      "[[ 2303  4832]\n",
      " [ 1286 37881]]\n",
      "('\\nAverage accuracy: ', 0.86599930888514542)\n",
      "\n",
      "\n",
      "Now testing penalty: l2\n",
      "====Iteration 0  ====\n",
      "accuracy 0.864865448577\n",
      "confusion matrix: \n",
      "[[ 2288  4871]\n",
      " [ 1386 37757]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.865815731502\n",
      "confusion matrix: \n",
      "[[ 2330  4779]\n",
      " [ 1434 37759]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.866917195801\n",
      "confusion matrix: \n",
      "[[ 2286  4863]\n",
      " [ 1299 37854]]\n",
      "====Iteration 3  ====\n",
      "accuracy 0.866830806445\n",
      "confusion matrix: \n",
      "[[ 2296  4855]\n",
      " [ 1311 37840]]\n",
      "====Iteration 4  ====\n",
      "accuracy 0.866593235713\n",
      "confusion matrix: \n",
      "[[ 2266  4819]\n",
      " [ 1358 37859]]\n",
      "====Iteration 5  ====\n",
      "accuracy 0.866053302233\n",
      "confusion matrix: \n",
      "[[ 2251  4829]\n",
      " [ 1373 37849]]\n",
      "====Iteration 6  ====\n",
      "accuracy 0.867478726621\n",
      "confusion matrix: \n",
      "[[ 2337  4831]\n",
      " [ 1305 37829]]\n",
      "====Iteration 7  ====\n",
      "accuracy 0.865383784718\n",
      "confusion matrix: \n",
      "[[ 2210  4812]\n",
      " [ 1421 37859]]\n",
      "====Iteration 8  ====\n",
      "accuracy 0.86613969159\n",
      "confusion matrix: \n",
      "[[ 2312  4801]\n",
      " [ 1397 37792]]\n",
      "====Iteration 9  ====\n",
      "accuracy 0.867305947907\n",
      "confusion matrix: \n",
      "[[ 2303  4759]\n",
      " [ 1385 37855]]\n",
      "('\\nAverage accuracy: ', 0.86633838711070799)\n"
     ]
    }
   ],
   "source": [
    "penalties = ('l1', 'l2')\n",
    "for p in penalties:\n",
    "    print ('\\n\\nNow testing penalty:'), p\n",
    "    \n",
    "    lr_clf = LogisticRegression(penalty=p, C=1.0, class_weight=None) # get object\n",
    "\n",
    "    iter_num = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    # the indices are the rows used for training and testing in each iteration\n",
    "    for train_indices, test_indices in cv_object: \n",
    "        X_train = X[train_indices]\n",
    "        y_train = y[train_indices]\n",
    "\n",
    "        X_test = X[test_indices]\n",
    "        y_test = y[test_indices]\n",
    "\n",
    "        # train the reusable logisitc regression model on the training data\n",
    "        lr_clf.fit(X_train,y_train)  # train object\n",
    "        y_hat = lr_clf.predict(X_test) # get test set predictions\n",
    "\n",
    "        # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "        acc = mt.accuracy_score(y_test,y_hat)\n",
    "        conf = mt.confusion_matrix(y_test,y_hat)\n",
    "        print \"====Iteration\",iter_num,\" ====\"\n",
    "        print \"accuracy\", acc\n",
    "        print \"confusion matrix: \\n\",conf\n",
    "        iter_num+=1\n",
    "        accuracy = accuracy + acc\n",
    "\n",
    "    print ('\\nAverage accuracy: ', accuracy/iter_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:brown\">The accuracy between L1 (0.866) and L2 (0.8663) is very small. This is small enough for our purposes that it doesn't make a difference which penalty model we use.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import k-fold cross validation from scikit learn\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "if 'health' in df_logreg:\n",
    "    y = df_logreg['health'].values # get the labels we want\n",
    "    del df_logreg['health'] # get ride of the class label\n",
    "    X = df_logreg.values # use everything else to predict!\n",
    "    \n",
    "    \n",
    "KFoldCrossObject = KFold(len(y), n_folds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('accuracy:', 0.84868250539956802)\n",
      "[[ 1337  2760]\n",
      " [  743 18310]]\n",
      "(u'HLTHPLN1', 'has weight of', 0.10941210180101903)\n",
      "(u'_IMPRACE', 'has weight of', -0.11970276363268452)\n",
      "(u'_SMOKER3', 'has weight of', 0.19378876359378444)\n",
      "(u'MEDCOST', 'has weight of', 0.23031960233356977)\n",
      "(u'EDUCA', 'has weight of', 0.35610806026835989)\n",
      "(u'_AGE_G', 'has weight of', -0.5230897017438727)\n",
      "(u'_INCOMG', 'has weight of', 0.64486151092982669)\n",
      "(u'PHYSHLTH', 'has weight of', 0.73342638152453898)\n"
     ]
    }
   ],
   "source": [
    "for train_indices, test_indices in KFoldCrossObject: \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "\n",
    "# scale attributes by the training set\n",
    "scale = StandardScaler()\n",
    "scale.fit(X_train) # find scalings for each column that make this zero mean and unit std\n",
    "\n",
    "X_train_scaled = scale.transform(X_train) # apply to training\n",
    "X_test_scaled = scale.transform(X_test) # apply those means and std to the test set (without snooping at the test set values)\n",
    "\n",
    "# train the model just as before\n",
    "logReg = LogisticRegression(penalty='l2', C=0.05, n_jobs=-1) \n",
    "logReg.fit(X_train_scaled,y_train)  # train object\n",
    "\n",
    "y_hat = logReg.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf )\n",
    "\n",
    "# sort these attributes and spit them out\n",
    "zip_vars = zip(logReg.coef_.T,df_logreg.columns) # combine attributes\n",
    "zip_vars.sort(key = lambda t: np.abs(t[0])) # sort them by the magnitude of the weight\n",
    "for coef, name in zip_vars:\n",
    "    print(name, 'has weight of', coef[0]) # now print them out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:brown\">Accuracy here is 84.9%, which is a little worse than above, but our cross validation step was added here.\n",
    "Looking at our data, we can see the weights of the different variables in the text above, or graphed below. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA24AAAEvCAYAAAA9ypKHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVLj/ARAyMpmZCbD5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18cee828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[15,4])\n",
    "\n",
    "weights = pd.Series(logReg.coef_[0],index=df_logreg.columns)\n",
    "weights.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Regression\n",
    "\n",
    "We've identified the relative weighting of the variables and their impact on self-reported health quality, but what if we could narrow it down from 9 variables to one? We'll pursue gradient boosting regression to understand if one of the variables could serve as a proxy for all others.\n",
    "\n",
    "In looking at the graphic above, it looks like the amount of exercise someone gets has the biggest impact on quality of health. So we’ll select that as the explanatory variable and keep self-reported health quality as our response variable. We’ll set the number of regression trees to 500, the depth of each individual tree to 4 the loss function to least squares and the learning rate to .01. The resulting mean squared error is .0859, which indicates a lack of fit between self-reported health quality and physical health. It would appear as though we need more variables in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.0859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x2e01b7f0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAGJCAYAAACThGjuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x42826438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# code built from: \n",
    "# http://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regression.html#example-ensemble-plot-gradient-boosting-regression-py\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "###############################################################################\n",
    "# Load data\n",
    "X, y = shuffle(df_reduced1.PHYSHLTH.reshape(-1,1), df_reduced1.health.ravel(), random_state=13)\n",
    "offset = int(X.shape[0] * 0.9)\n",
    "X_train, y_train = X[:offset], y[:offset]\n",
    "X_test, y_test = X[offset:], y[offset:]\n",
    "\n",
    "###############################################################################\n",
    "# Fit regression model\n",
    "params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 1,\n",
    "          'learning_rate': 0.01, 'loss': 'ls'}\n",
    "clf = ensemble.GradientBoostingRegressor(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "mse = mean_squared_error(y_test, clf.predict(X_test))\n",
    "print(\"MSE: %.4f\" % mse)\n",
    "\n",
    "###############################################################################\n",
    "# Plot training deviance\n",
    "\n",
    "# compute test set deviance\n",
    "test_score = np.zeros((params['n_estimators'],), dtype=np.float64)\n",
    "\n",
    "for i, y_pred in enumerate(clf.staged_predict(X_test)):\n",
    "    test_score[i] = clf.loss_(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Deviance')\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, clf.train_score_, 'b-',\n",
    "         label='Training Set Deviance')\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-',\n",
    "         label='Test Set Deviance')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Boosting Iterations')\n",
    "plt.ylabel('Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span syle=\"color:brown\"> *** Section added ***</span>\n",
    "### Interpreting the Chart Above\n",
    "\n",
    "The chart above shows a very small difference between our training and test data set, which is a good thing, but the very low MSE means a lack of fit between the variables and an overall weak predictive model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# K-Nearest Neighbors\n",
    "---\n",
    "\n",
    "In this section, will do an prediction analysis based on k-nearest neighbors (KNN).\n",
    "This should give us clusters based on how well each point clusters on others.\n",
    "\n",
    "<span style=\"color=brown\">We try several values for K, [3, 5, 10, 20], in order to see if there is a difference between using a few neighbors and using a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  1.,   3.,  88., ...,   2.,   1.,   1.],\n",
       "        [  1.,   3.,   2., ...,   2.,   1.,   1.],\n",
       "        [  1.,   4.,   3., ...,   2.,   1.,   1.],\n",
       "        ..., \n",
       "        [  0.,   3.,  20., ...,   2.,   1.,   5.],\n",
       "        [  1.,   4.,  88., ...,   2.,   1.,   5.],\n",
       "        [  1.,   4.,  88., ...,   2.,   1.,   5.]]),\n",
       " array([ 1.,  1.,  1., ...,  1.,  0.,  1.]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial setup\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "knn = df_reduced  # Copy the dataset for this analysis.\n",
    "knn.fillna(value=0)\n",
    "\n",
    "# and setup our X and Y\n",
    "if '_Health' in knn:\n",
    "    y = knn['_Health'].values # get the labels we want\n",
    "    del knn['_Health'] # get rid of the class label\n",
    "\n",
    "yhat = np.zeros(y.shape)  #empty array to fill with predictions m\n",
    "X = knn.values\n",
    "\n",
    "# Display the arrays.  Not really needed, but tells us that there are some nan values, which break things.\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data Cleanup. In conversion to numpy array from pandas\n",
    "# some values are being converted to nan or inf values.\n",
    "X=np.nan_to_num(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 KNN accuracy 0.795332322565\n",
      "5 KNN accuracy 0.818744141646\n",
      "10 KNN accuracy 0.833521232619\n",
      "20 KNN accuracy 0.845840514542\n"
     ]
    }
   ],
   "source": [
    "#fit the KNN model to our data\n",
    "cv = StratifiedKFold(y, n_folds=10)\n",
    "\n",
    "# get a handle to the classifier object, which defines the type\n",
    "for nn in (3, 5, 10, 20):\n",
    "\n",
    "    clf = KNeighborsClassifier(n_neighbors=nn)\n",
    "\n",
    "    # now iterate through and get predictions, saved to the correct row in yhat\n",
    "    for train, test in cv:\n",
    "        clf.fit(X[train],y[train])\n",
    "        yhat[test] = clf.predict(X[test])\n",
    "\n",
    "    total_accuracy = mt.accuracy_score(y, yhat)\n",
    "    print nn,'KNN accuracy', total_accuracy\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 KNN accuracy 0.845935544066\n",
      "50 KNN accuracy 0.845957141685\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Based on above, what if we go higher than 20?\n",
    "\n",
    "cv = StratifiedKFold(y, n_folds=10)\n",
    "\n",
    "# get a handle to the classifier object, which defines the type\n",
    "for nn in (30, 50):\n",
    "\n",
    "    clf = KNeighborsClassifier(n_neighbors=nn)\n",
    "\n",
    "    # now iterate through and get predictions, saved to the correct row in yhat\n",
    "    for train, test in cv:\n",
    "        clf.fit(X[train],y[train])\n",
    "        yhat[test] = clf.predict(X[test])\n",
    "\n",
    "    total_accuracy = mt.accuracy_score(y, yhat)\n",
    "    print nn,'KNN accuracy', total_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:brown\">Interesting, larger values of K don't add much for the accuracy. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Pipeline with PCA and KNN\n",
    "---\n",
    "\n",
    "<span style=\"color:brown\">The next question is can we improve on KNN? How accurate are our values?</span> Now trying a pipeline doing a PCA first, then the KNN again. (Again, with k=10.)\n",
    "Expect it will get even better, since PCA should eliminate whats no good. However, at this point we need to watch for overfitting. if our accuracy jumps real high, we should be suspicious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN, pipeline accuracy 0.836752236433\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import RandomizedPCA \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# setup pipeline to take PCA, then fit a KNN classifier\n",
    "clf_pipe = Pipeline(\n",
    "    [('PCA',RandomizedPCA(n_components=300)),\n",
    "     ('CLF',KNeighborsClassifier(n_neighbors=10))]\n",
    ")\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "for train, test in cv:\n",
    "    clf_pipe.fit(X[train],y[train])\n",
    "    yhat[test] = clf_pipe.predict(X[test])\n",
    "\n",
    "total_accuracy = mt.accuracy_score(y, yhat)\n",
    "print 'KNN, pipeline accuracy', total_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:brown\">This is in the same ballpark as the KNN run above.  We were expecting slightly better, but for all practical purposes, this is fine.</span>\n",
    "\n",
    "Let's try something else..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Random Forest Classifier\n",
    "--- \n",
    "Since two different versions of KNN came out the same, lets compare it to something else.  We'll try random forest in our pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_pipe = Pipeline(\n",
    "    [('PCA',RandomizedPCA(n_components=100)),\n",
    "     ('CLF',RandomForestClassifier(max_depth=50, n_estimators=150, n_jobs=-1))]\n",
    ")\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "for train, test in cv:\n",
    "    clf_pipe.fit(X[train],y[train])\n",
    "    yhat[test] = clf_pipe.predict(X[test])\n",
    "    \n",
    "total_accuracy = mt.accuracy_score(y, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline accuracy: 0.836860224529\n"
     ]
    }
   ],
   "source": [
    "print 'Pipeline accuracy:', mt.accuracy_score(y, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is approximately what we had with the other classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*** Section Added ***\n",
    "\n",
    "### Deployment\n",
    "\n",
    "For individuals, the model helps to clarify the relationship between self-reported health quality and a number of demographics and behaviors. The demographics are obviously beyond a person’s control, but the behaviors are not. The model shows that education level has the strongest correlation to good or better self-reported health quality, which we didn’t anticipate. Even if we can’t go so far as to interpret a cause and effect relationship, it appears as though those with more education self-report higher health quality. \n",
    "\n",
    "For policy makers, there is a constant competition for available public funds. If the case can be made that education not only helps a person to be more successful in their life, but also has a strong correlation to health quality, it could better make the case for increased investment. \n",
    "\n",
    "Many of these variables change over time and since the survey is conducted on an annual basis, it would be important to update the model each year and observe any changes between the relative weighting of the variables in the model. \n",
    "\n",
    "*If the intent of the survey, the instrument where all this data came from, was to predict health quality, the surveyors could theoretically shorten the survey from 200+ variables to these key 9 and achieve similar results, at a much lower cost.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
